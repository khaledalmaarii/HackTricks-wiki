<details>

<summary><a href="https://cloud.hacktricks.xyz/pentesting-cloud/pentesting-cloud-methodology"><strong>â˜ï¸ HackTricksäº‘ â˜ï¸</strong></a> -<a href="https://twitter.com/hacktricks_live"><strong>ğŸ¦ æ¨ç‰¹ ğŸ¦</strong></a> - <a href="https://www.twitch.tv/hacktricks_live/schedule"><strong>ğŸ™ï¸ Twitch ğŸ™ï¸</strong></a> - <a href="https://www.youtube.com/@hacktricks_LIVE"><strong>ğŸ¥ Youtube ğŸ¥</strong></a></summary>

- ä½ åœ¨ä¸€å®¶**ç½‘ç»œå®‰å…¨å…¬å¸**å·¥ä½œå—ï¼Ÿä½ æƒ³åœ¨HackTricksä¸­çœ‹åˆ°ä½ çš„**å…¬å¸å¹¿å‘Š**å—ï¼Ÿæˆ–è€…ä½ æƒ³è·å¾—**PEASSçš„æœ€æ–°ç‰ˆæœ¬æˆ–ä¸‹è½½HackTricksçš„PDF**å—ï¼Ÿè¯·æŸ¥çœ‹[**è®¢é˜…è®¡åˆ’**](https://github.com/sponsors/carlospolop)ï¼

- å‘ç°æˆ‘ä»¬çš„ç‹¬å®¶[**NFTs**](https://opensea.io/collection/the-peass-family)æ”¶è—å“[**The PEASS Family**](https://opensea.io/collection/the-peass-family)

- è·å¾—[**å®˜æ–¹PEASSå’ŒHackTrickså‘¨è¾¹äº§å“**](https://peass.creator-spring.com)

- **åŠ å…¥** [**ğŸ’¬**](https://emojipedia.org/speech-balloon/) [**Discordç¾¤ç»„**](https://discord.gg/hRep4RUj7f) æˆ– [**Telegramç¾¤ç»„**](https://t.me/peass) æˆ– **å…³æ³¨**æˆ‘åœ¨**Twitter**ä¸Šçš„[**ğŸ¦**](https://github.com/carlospolop/hacktricks/tree/7af18b62b3bdc423e11444677a6a73d4043511e9/\[https:/emojipedia.org/bird/README.md)[**@carlospolopm**](https://twitter.com/hacktricks_live)**ã€‚**

- **é€šè¿‡å‘[hacktricks repo](https://github.com/carlospolop/hacktricks)å’Œ[hacktricks-cloud repo](https://github.com/carlospolop/hacktricks-cloud)æäº¤PRæ¥åˆ†äº«ä½ çš„é»‘å®¢æŠ€å·§**ã€‚

</details>


# å¯èƒ½çš„åŸºæœ¬æ•°æ®ç±»å‹

æ•°æ®å¯ä»¥æ˜¯**è¿ç»­å‹**ï¼ˆ**æ— é™**ä¸ªå€¼ï¼‰æˆ–**åˆ†ç±»å‹**ï¼ˆåä¹‰ï¼‰å…¶ä¸­å¯èƒ½çš„å€¼çš„æ•°é‡æ˜¯**æœ‰é™**çš„ã€‚

## åˆ†ç±»å‹æ•°æ®ç±»å‹

### äºŒè¿›åˆ¶

åªæœ‰**2ä¸ªå¯èƒ½çš„å€¼**ï¼š1æˆ–0ã€‚å¦‚æœæ•°æ®é›†ä¸­çš„å€¼æ˜¯å­—ç¬¦ä¸²æ ¼å¼ï¼ˆä¾‹å¦‚"True"å’Œ"False"ï¼‰ï¼Œåˆ™å¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ–¹å¼å°†è¿™äº›å€¼åˆ†é…ä¸ºæ•°å­—ï¼š
```python
dataset["column2"] = dataset.column2.map({"T": 1, "F": 0})
```
### **æœ‰åº**

**å€¼æŒ‰ç…§ä¸€å®šé¡ºåºæ’åˆ—**ï¼Œä¾‹å¦‚ï¼šç¬¬ä¸€åï¼Œç¬¬äºŒå... å¦‚æœç±»åˆ«æ˜¯å­—ç¬¦ä¸²ï¼ˆä¾‹å¦‚ï¼š"åˆå­¦è€…"ï¼Œ"ä¸šä½™çˆ±å¥½è€…"ï¼Œ"ä¸“ä¸šäººå£«"ï¼Œ"ä¸“å®¶"ï¼‰ï¼Œä½ å¯ä»¥åƒåœ¨äºŒè¿›åˆ¶æƒ…å†µä¸‹é‚£æ ·å°†å®ƒä»¬æ˜ å°„ä¸ºæ•°å­—ã€‚
```python
column2_mapping = {'starter':0,'amateur':1,'professional':2,'expert':3}
dataset['column2'] = dataset.column2.map(column2_mapping)
```
* å¯¹äº**å­—æ¯åˆ—**ï¼Œæ‚¨å¯ä»¥æ›´è½»æ¾åœ°å¯¹å…¶è¿›è¡Œæ’åºï¼š
```python
# First get all the uniq values alphabetically sorted
possible_values_sorted = dataset.column2.sort_values().unique().tolist()
# Assign each one a value
possible_values_mapping = {value:idx for idx,value in enumerate(possible_values_sorted)}
dataset['column2'] = dataset.column2.map(possible_values_mapping)
```
### **å¾ªç¯ç‰¹å¾**

çœ‹èµ·æ¥åƒæ˜¯æœ‰åºå€¼ï¼Œå› ä¸ºæœ‰ä¸€ç§é¡ºåºï¼Œä½†å¹¶ä¸æ„å‘³ç€ä¸€ä¸ªæ¯”å¦ä¸€ä¸ªå¤§ã€‚è€Œä¸”å®ƒä»¬ä¹‹é—´çš„è·ç¦»å–å†³äºä½ è®¡æ•°çš„æ–¹å‘ã€‚ä¾‹å¦‚ï¼šä¸€å‘¨çš„å¤©æ•°ï¼Œæ˜ŸæœŸå¤©å¹¶ä¸æ¯”æ˜ŸæœŸä¸€â€œå¤§â€ã€‚

* æœ‰ä¸åŒçš„æ–¹æ³•æ¥ç¼–ç å¾ªç¯ç‰¹å¾ï¼Œå…¶ä¸­ä¸€äº›å¯èƒ½åªé€‚ç”¨äºæŸäº›ç®—æ³•ã€‚**é€šå¸¸æƒ…å†µä¸‹ï¼Œå¯ä»¥ä½¿ç”¨è™šæ‹Ÿç¼–ç **ã€‚
```python
column2_dummies = pd.get_dummies(dataset.column2, drop_first=True)
dataset_joined = pd.concat([dataset[['column2']], column2_dummies], axis=1)
```
### **æ—¥æœŸ**

æ—¥æœŸæ˜¯**è¿ç»­**çš„**å˜é‡**ã€‚å¯ä»¥è¢«è§†ä¸º**å¾ªç¯**çš„ï¼ˆå› ä¸ºå®ƒä»¬é‡å¤å‡ºç°ï¼‰æˆ–è€…ä½œä¸º**æœ‰åº**å˜é‡ï¼ˆå› ä¸ºä¸€ä¸ªæ—¶é—´ç‚¹æ¯”å‰ä¸€ä¸ªæ—¶é—´ç‚¹å¤§ï¼‰ã€‚

* é€šå¸¸æ—¥æœŸè¢«ç”¨ä½œ**ç´¢å¼•**
```python
# Transform dates to datetime
dataset["column_date"] = pd.to_datetime(dataset.column_date)
# Make the date feature the index
dataset.set_index('column_date', inplace=True)
print(dataset.head())

# Sum usage column per day
daily_sum = dataset.groupby(df_daily_usage.index.date).agg({'usage':['sum']})
# Flatten and rename usage column
daily_sum.columns = daily_sum.columns.get_level_values(0)
daily_sum.columns = ['daily_usage']
print(daily_sum.head())

# Fill days with 0 usage
idx = pd.date_range('2020-01-01', '2020-12-31')
daily_sum.index = pd.DatetimeIndex(daily_sum.index)
df_filled = daily_sum.reindex(idx, fill_value=0) # Fill missing values


# Get day of the week, Monday=0, Sunday=6, and week days names
dataset['DoW'] = dataset.transaction_date.dt.dayofweek
# do the same in a different way
dataset['weekday'] = dataset.transaction_date.dt.weekday
# get day names
dataset['day_name'] = dataset.transaction_date.apply(lambda x: x.day_name())
```
### å¤šç±»åˆ«/åä¹‰

**è¶…è¿‡2ä¸ªç±»åˆ«**ï¼Œæ²¡æœ‰ç›¸å…³é¡ºåºã€‚ä½¿ç”¨ `dataset.describe(include='all')` è·å–æ¯ä¸ªç‰¹å¾çš„ç±»åˆ«ä¿¡æ¯ã€‚

* **å¼•ç”¨å­—ç¬¦ä¸²**æ˜¯ä¸€ä¸ª**æ ‡è¯†ç¤ºä¾‹çš„åˆ—**ï¼ˆæ¯”å¦‚ä¸€ä¸ªäººçš„åå­—ï¼‰ã€‚è¿™å¯èƒ½ä¼šé‡å¤ï¼ˆå› ä¸ºä¸¤ä¸ªäººå¯èƒ½æœ‰ç›¸åŒçš„åå­—ï¼‰ï¼Œä½†å¤§å¤šæ•°æ˜¯å”¯ä¸€çš„ã€‚è¿™äº›æ•°æ®æ˜¯**æ— ç”¨çš„ï¼Œåº”è¯¥è¢«åˆ é™¤**ã€‚
* **å…³é”®åˆ—**ç”¨äº**é“¾æ¥è¡¨ä¹‹é—´çš„æ•°æ®**ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå…ƒç´ æ˜¯å”¯ä¸€çš„ã€‚è¿™äº›æ•°æ®æ˜¯**æ— ç”¨çš„ï¼Œåº”è¯¥è¢«åˆ é™¤**ã€‚

ä¸ºäº†å°†**å¤šç±»åˆ«åˆ—ç¼–ç ä¸ºæ•°å­—**ï¼ˆä»¥ä¾¿æœºå™¨å­¦ä¹ ç®—æ³•ç†è§£å®ƒä»¬ï¼‰ï¼Œä½¿ç”¨**è™šæ‹Ÿç¼–ç **ï¼ˆè€Œä¸æ˜¯ç‹¬çƒ­ç¼–ç ï¼Œå› ä¸ºå®ƒ**ä¸èƒ½é¿å…å®Œç¾å¤šé‡å…±çº¿æ€§**ï¼‰ã€‚

ä½ å¯ä»¥ä½¿ç”¨ `pd.get_dummies(dataset.column1)` æ¥è·å–**å¤šç±»åˆ«åˆ—çš„ç‹¬çƒ­ç¼–ç **ã€‚è¿™å°†æŠŠæ‰€æœ‰ç±»åˆ«è½¬æ¢ä¸ºäºŒè¿›åˆ¶ç‰¹å¾ï¼Œå› æ­¤ä¼šåˆ›å»º**æ¯ä¸ªå¯èƒ½ç±»åˆ«çš„æ–°åˆ—**ï¼Œå¹¶å°†1åˆ†é…ç»™**ä¸€ä¸ªåˆ—çš„Trueå€¼**ï¼Œå…¶ä½™åˆ—ä¸ºfalseã€‚

ä½ å¯ä»¥ä½¿ç”¨ `pd.get_dummies(dataset.column1, drop_first=True)` æ¥è·å–**å¤šç±»åˆ«åˆ—çš„è™šæ‹Ÿç¼–ç **ã€‚è¿™å°†æŠŠæ‰€æœ‰ç±»åˆ«è½¬æ¢ä¸ºäºŒè¿›åˆ¶ç‰¹å¾ï¼Œå› æ­¤ä¼šåˆ›å»º**æ¯ä¸ªå¯èƒ½ç±»åˆ«å‡ä¸€çš„æ–°åˆ—**ï¼Œæœ€åä¸¤åˆ—å°†åœ¨æœ€åä¸€ä¸ªäºŒè¿›åˆ¶åˆ—ä¸­åæ˜ ä¸º"1"æˆ–"0"ã€‚è¿™å°†é¿å…å®Œç¾å¤šé‡å…±çº¿æ€§ï¼Œå‡å°‘åˆ—ä¹‹é—´çš„å…³ç³»ã€‚

# å…±çº¿æ€§/å¤šé‡å…±çº¿æ€§

å½“**ä¸¤ä¸ªç‰¹å¾å½¼æ­¤ç›¸å…³**æ—¶å‡ºç°å…±çº¿æ€§ã€‚å½“è¶…è¿‡2ä¸ªç‰¹å¾ç›¸å…³æ—¶å‡ºç°å¤šé‡å…±çº¿æ€§ã€‚

åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œ**ä½ å¸Œæœ›ç‰¹å¾ä¸å¯èƒ½çš„ç»“æœç›¸å…³ï¼Œä½†ä¸å¸Œæœ›å®ƒä»¬å½¼æ­¤ç›¸å…³**ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆè™šæ‹Ÿç¼–ç æ··åˆäº†æœ€åä¸¤åˆ—ï¼Œå¹¶ä¸”**æ¯”ç‹¬çƒ­ç¼–ç æ›´å¥½**ï¼Œå› ä¸ºç‹¬çƒ­ç¼–ç æ²¡æœ‰è¿™æ ·åšï¼Œä»è€Œåœ¨å¤šç±»åˆ«åˆ—çš„æ‰€æœ‰æ–°ç‰¹å¾ä¹‹é—´åˆ›å»ºäº†æ˜ç¡®çš„å…³ç³»ã€‚

VIFæ˜¯**æ–¹å·®è†¨èƒ€å› å­**ï¼Œç”¨äº**è¡¡é‡ç‰¹å¾ä¹‹é—´çš„å¤šé‡å…±çº¿æ€§**ã€‚å€¼**å¤§äº5æ„å‘³ç€åº”è¯¥åˆ é™¤ä¸¤ä¸ªæˆ–å¤šä¸ªå…±çº¿ç‰¹å¾ä¸­çš„ä¸€ä¸ª**ã€‚
```python
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

#dummies_encoded = pd.get_dummies(dataset.column1, drop_first=True)
onehot_encoded = pd.get_dummies(dataset.column1)
X = add_constant(onehot_encoded) # Add previously one-hot encoded data
print(pd.Series([variance_inflation_factor(X.values,i) for i in range(X.shape[1])], index=X.columns))
```
# ç±»åˆ«ä¸å¹³è¡¡

å½“è®­ç»ƒæ•°æ®ä¸­çš„æ¯ä¸ªç±»åˆ«çš„æ•°é‡ä¸ç›¸ç­‰æ—¶ï¼Œå°±ä¼šå‡ºç°ç±»åˆ«ä¸å¹³è¡¡çš„æƒ…å†µã€‚
```python
# Get statistic of the features
print(dataset.describe(include='all'))
# Get an overview of the features
print(dataset.info())
# Get imbalance information of the target column
print(dataset.target_column.value_counts())
```
åœ¨ä¸å¹³è¡¡çš„æƒ…å†µä¸‹ï¼Œæ€»ä¼šå­˜åœ¨**å¤šæ•°ç±»åˆ«**å’Œ**å°‘æ•°ç±»åˆ«**ã€‚

è§£å†³è¿™ä¸ªé—®é¢˜æœ‰ä¸¤ç§ä¸»è¦æ–¹æ³•ï¼š

* **æ¬ é‡‡æ ·**ï¼šä»å¤šæ•°ç±»åˆ«ä¸­éšæœºåˆ é™¤æ•°æ®ï¼Œä½¿å…¶ä¸å°‘æ•°ç±»åˆ«å…·æœ‰ç›¸åŒæ•°é‡çš„æ ·æœ¬ã€‚
```python
from imblearn.under_sampling import RandomUnderSampler
rus = RandomUserSampler(random_state=1337)

X = dataset[['column1', 'column2', 'column3']].copy()
y = dataset.target_column

X_under, y_under = rus.fit_resample(X,y)
print(y_under.value_counts()) #Confirm data isn't imbalanced anymore
```
* **è¿‡é‡‡æ ·**ï¼šä¸ºå°‘æ•°ç±»åˆ«ç”Ÿæˆæ›´å¤šçš„æ•°æ®ï¼Œç›´åˆ°å…¶æ ·æœ¬æ•°é‡ä¸å¤šæ•°ç±»åˆ«ç›¸åŒã€‚
```python
from imblearn.under_sampling import RandomOverSampler
ros = RandomOverSampler(random_state=1337)

X = dataset[['column1', 'column2', 'column3']].copy()
y = dataset.target_column

X_over, y_over = ros.fit_resample(X,y)
print(y_over.value_counts()) #Confirm data isn't imbalanced anymore
```
æ‚¨å¯ä»¥ä½¿ç”¨å‚æ•°**`sampling_strategy`**æ¥æŒ‡ç¤ºæ‚¨æƒ³è¦è¿›è¡Œ**æ¬ é‡‡æ ·æˆ–è¿‡é‡‡æ ·**çš„**ç™¾åˆ†æ¯”**ï¼ˆé»˜è®¤ä¸º1ï¼ˆ100%ï¼‰ï¼Œæ„å‘³ç€å°†å°‘æ•°ç±»åˆ«çš„æ•°é‡ä¸å¤šæ•°ç±»åˆ«ç›¸ç­‰ï¼‰ã€‚

{% hint style="info" %}
æ¬ é‡‡æ ·æˆ–è¿‡é‡‡æ ·å¹¶ä¸å®Œç¾ï¼Œå¦‚æœæ‚¨ä½¿ç”¨`.describe()`è·å–è¿‡/æ¬ é‡‡æ ·æ•°æ®çš„ç»Ÿè®¡ä¿¡æ¯å¹¶å°†å…¶ä¸åŸå§‹æ•°æ®è¿›è¡Œæ¯”è¾ƒï¼Œæ‚¨ä¼šå‘ç°å®ƒä»¬å·²ç»å‘ç”Ÿäº†**å˜åŒ–**ã€‚å› æ­¤ï¼Œè¿‡é‡‡æ ·å’Œæ¬ é‡‡æ ·ä¼šä¿®æ”¹è®­ç»ƒæ•°æ®ã€‚
{% endhint %}

## SMOTEè¿‡é‡‡æ ·

**SMOTE**é€šå¸¸æ˜¯ä¸€ç§**æ›´å¯é çš„è¿‡é‡‡æ ·æ•°æ®çš„æ–¹æ³•**ã€‚
```python
from imblearn.over_sampling import SMOTE

# Form SMOTE the target_column need to be numeric, map it if necessary
smote = SMOTE(random_state=1337)
X_smote, y_smote = smote.fit_resample(dataset[['column1', 'column2', 'column3']], dataset.target_column)
dataset_smote = pd.DataFrame(X_smote, columns=['column1', 'column2', 'column3'])
dataset['target_column'] = y_smote
print(y_smote.value_counts()) #Confirm data isn't imbalanced anymore
```
# å¾ˆå°‘å‡ºç°çš„ç±»åˆ«

æƒ³è±¡ä¸€ä¸‹ä¸€ä¸ªæ•°æ®é›†ï¼Œå…¶ä¸­ä¸€ä¸ªç›®æ ‡ç±»åˆ«**å‡ºç°çš„æ¬¡æ•°éå¸¸å°‘**ã€‚

è¿™å°±åƒå‰ä¸€èŠ‚ä¸­çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œä½†æ˜¯åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¾ˆå°‘å‡ºç°çš„ç±»åˆ«ç”šè‡³æ¯”"å°‘æ•°ç±»"å‡ºç°çš„æ¬¡æ•°è¿˜è¦å°‘ã€‚åœ¨è¿™é‡Œä¹Ÿå¯ä»¥ä½¿ç”¨**åŸå§‹**çš„**è¿‡é‡‡æ ·**å’Œ**æ¬ é‡‡æ ·**æ–¹æ³•ï¼Œä½†é€šå¸¸è¿™äº›æŠ€æœ¯**ä¸ä¼šç»™å‡ºéå¸¸å¥½çš„ç»“æœ**ã€‚

## æƒé‡

åœ¨æŸäº›ç®—æ³•ä¸­ï¼Œå¯ä»¥**ä¿®æ”¹ç›®æ ‡æ•°æ®çš„æƒé‡**ï¼Œä»¥ä¾¿åœ¨ç”Ÿæˆæ¨¡å‹æ—¶é»˜è®¤æ›´é‡è¦ã€‚
```python
weights = {0: 10 1:1} #Assign weight 10 to False and 1 to True
model = LogisticRegression(class_weight=weights)
```
æ‚¨å¯ä»¥ä½¿ç”¨è¿‡/æ¬ é‡‡æ ·æŠ€æœ¯ä¸æƒé‡æ··åˆæ¥å°è¯•æ”¹å–„ç»“æœã€‚

## PCA - ä¸»æˆåˆ†åˆ†æ

è¿™æ˜¯ä¸€ç§å¸®åŠ©é™ä½æ•°æ®ç»´åº¦çš„æ–¹æ³•ã€‚å®ƒå°†**ç»„åˆä¸åŒçš„ç‰¹å¾**ä»¥**å‡å°‘ç‰¹å¾æ•°é‡**ï¼Œä»è€Œç”Ÿæˆ**æ›´æœ‰ç”¨çš„ç‰¹å¾**ï¼ˆéœ€è¦æ›´å°‘çš„è®¡ç®—ï¼‰ã€‚

ç”Ÿæˆçš„ç‰¹å¾å¯¹äººç±»æ¥è¯´æ˜¯ä¸å¯ç†è§£çš„ï¼Œå› æ­¤å®ƒè¿˜**åŒ¿ååŒ–æ•°æ®**ã€‚

# ä¸ä¸€è‡´çš„æ ‡ç­¾ç±»åˆ«

æ•°æ®å¯èƒ½å­˜åœ¨è½¬æ¢å¤±è´¥æˆ–äººä¸ºé”™è¯¯å¯¼è‡´çš„é”™è¯¯ã€‚

å› æ­¤ï¼Œæ‚¨å¯èƒ½ä¼šå‘ç°**ç›¸åŒçš„æ ‡ç­¾å­˜åœ¨æ‹¼å†™é”™è¯¯**ï¼Œä¸åŒçš„**å¤§å°å†™**ï¼Œ**ç¼©å†™**ï¼Œä¾‹å¦‚ï¼š_BLUEï¼ŒBlueï¼Œbï¼Œbule_ã€‚åœ¨è®­ç»ƒæ¨¡å‹ä¹‹å‰ï¼Œæ‚¨éœ€è¦ä¿®å¤æ•°æ®ä¸­çš„è¿™äº›æ ‡ç­¾é”™è¯¯ã€‚

æ‚¨å¯ä»¥é€šè¿‡å°†æ‰€æœ‰å†…å®¹è½¬æ¢ä¸ºå°å†™å¹¶å°†æ‹¼å†™é”™è¯¯çš„æ ‡ç­¾æ˜ å°„åˆ°æ­£ç¡®çš„æ ‡ç­¾æ¥æ¸…ç†è¿™äº›é—®é¢˜ã€‚

éå¸¸é‡è¦çš„æ˜¯æ£€æŸ¥**æ‚¨æ‹¥æœ‰çš„æ‰€æœ‰æ•°æ®æ˜¯å¦æ­£ç¡®æ ‡è®°**ï¼Œå› ä¸ºä¾‹å¦‚ï¼Œæ•°æ®ä¸­çš„ä¸€ä¸ªæ‹¼å†™é”™è¯¯ï¼Œåœ¨å¯¹ç±»åˆ«è¿›è¡Œè™šæ‹Ÿç¼–ç æ—¶ï¼Œå°†åœ¨æœ€ç»ˆç‰¹å¾ä¸­ç”Ÿæˆä¸€ä¸ªæ–°åˆ—ï¼Œå¯¹æœ€ç»ˆæ¨¡å‹äº§ç”Ÿ**ä¸è‰¯åæœ**ã€‚é€šè¿‡å¯¹ä¸€åˆ—è¿›è¡Œç‹¬çƒ­ç¼–ç å¹¶æ£€æŸ¥æ‰€åˆ›å»ºçš„åˆ—çš„åç§°ï¼Œå¯ä»¥å¾ˆå®¹æ˜“åœ°æ£€æµ‹åˆ°æ­¤ç¤ºä¾‹ã€‚

# ç¼ºå¤±æ•°æ®

ç ”ç©¶ä¸­å¯èƒ½ç¼ºå°‘ä¸€äº›æ•°æ®ã€‚

å¯èƒ½ä¼šå‘ç”Ÿä¸€äº›å®Œå…¨éšæœºçš„æ•°æ®ä¸¢å¤±ï¼Œè¿™ç§æ•°æ®è¢«ç§°ä¸º**å®Œå…¨éšæœºç¼ºå¤±**ï¼ˆ**MCAR**ï¼‰ã€‚

å¯èƒ½ä¼šæœ‰ä¸€äº›éšæœºæ•°æ®ä¸¢å¤±ï¼Œä½†æŸäº›ç‰¹å®šç»†èŠ‚æ›´æœ‰å¯èƒ½ä¸¢å¤±ï¼Œä¾‹å¦‚ç”·æ€§æ›´æœ‰å¯èƒ½å‘Šè¯‰ä»–ä»¬çš„å¹´é¾„ï¼Œè€Œå¥³æ€§åˆ™ä¸ä¼šã€‚è¿™è¢«ç§°ä¸º**éšæœºç¼ºå¤±**ï¼ˆ**MAR**ï¼‰ã€‚

æœ€åï¼Œå¯èƒ½å­˜åœ¨**ééšæœºç¼ºå¤±**ï¼ˆ**MNAR**ï¼‰çš„æ•°æ®ã€‚æ•°æ®çš„å€¼ä¸å…·æœ‰æ•°æ®çš„æ¦‚ç‡ç›´æ¥ç›¸å…³ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æƒ³æµ‹é‡æŸäº›ä»¤äººå°´å°¬çš„äº‹æƒ…ï¼ŒæŸäººè¶Šå°´å°¬ï¼Œä»–åˆ†äº«çš„å¯èƒ½æ€§å°±è¶Šå°ã€‚

å‰ä¸¤ç§ç¼ºå¤±æ•°æ®ç±»åˆ«å¯ä»¥è¢«**å¿½ç•¥**ã€‚ä½†æ˜¯ç¬¬ä¸‰ç§éœ€è¦è€ƒè™‘**æœªå—å½±å“çš„æ•°æ®éƒ¨åˆ†**æˆ–å°è¯•**ä»¥æŸç§æ–¹å¼å¯¹ç¼ºå¤±æ•°æ®è¿›è¡Œå»ºæ¨¡**ã€‚

äº†è§£ç¼ºå¤±æ•°æ®çš„ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨`.info()`å‡½æ•°ï¼Œå®ƒå°†æŒ‡ç¤º**æ¯ä¸ªç±»åˆ«çš„è¡Œæ•°å’Œå€¼çš„æ•°é‡**ã€‚å¦‚æœæŸä¸ªç±»åˆ«çš„å€¼å°‘äºè¡Œæ•°ï¼Œåˆ™å­˜åœ¨ä¸€äº›ç¼ºå¤±æ•°æ®ï¼š
```bash
# Get info of the dataset
dataset.info()

# Drop all rows where some value is missing
dataset.dropna(how='any', axis=0).info()
```
é€šå¸¸å»ºè®®ï¼Œå¦‚æœæ•°æ®é›†ä¸­**ç¼ºå¤±çš„ç‰¹å¾è¶…è¿‡20%**ï¼Œåˆ™åº”è¯¥**åˆ é™¤è¯¥åˆ—**ï¼š
```bash
# Remove column
dataset.drop('Column_name', axis='columns', inplace=True)
dataset.info()
```
{% hint style="info" %}
è¯·æ³¨æ„ï¼Œ**æ•°æ®é›†ä¸­å¹¶éæ‰€æœ‰ç¼ºå¤±å€¼éƒ½æ˜¯ç¼ºå¤±çš„**ã€‚å¯èƒ½ç¼ºå¤±å€¼å·²ç»è¢«èµ‹äºˆäº†"Unknown"ã€"n/a"ã€""ã€-1ã€0ç­‰å€¼ã€‚æ‚¨éœ€è¦æ£€æŸ¥æ•°æ®é›†ï¼ˆä½¿ç”¨`dataset.column_name.value_counts(dropna=False)`æ¥æ£€æŸ¥å¯èƒ½çš„å€¼ï¼‰ã€‚
{% endhint %}

å¦‚æœæ•°æ®é›†ä¸­æœ‰ä¸€äº›æ•°æ®ç¼ºå¤±ï¼ˆæ•°é‡ä¸å¤ªå¤šï¼‰ï¼Œæ‚¨éœ€è¦æ‰¾åˆ°**ç¼ºå¤±æ•°æ®çš„ç±»åˆ«**ã€‚ä¸ºæ­¤ï¼Œæ‚¨åŸºæœ¬ä¸Šéœ€è¦çŸ¥é“**ç¼ºå¤±æ•°æ®æ˜¯å¦æ˜¯éšæœºçš„**ï¼Œè€Œè¦æ‰¾å‡ºè¿™ä¸€ç‚¹ï¼Œæ‚¨éœ€è¦æ‰¾å‡º**ç¼ºå¤±æ•°æ®æ˜¯å¦ä¸æ•°æ®é›†çš„å…¶ä»–æ•°æ®ç›¸å…³**ã€‚

è¦æ‰¾å‡ºç¼ºå¤±å€¼æ˜¯å¦ä¸å¦ä¸€åˆ—ç›¸å…³ï¼Œæ‚¨å¯ä»¥åˆ›å»ºä¸€ä¸ªæ–°åˆ—ï¼Œå¦‚æœæ•°æ®ç¼ºå¤±åˆ™å°†å…¶è®¾ä¸º1ï¼Œå¦åˆ™è®¾ä¸º0ï¼Œç„¶åè®¡ç®—å®ƒä»¬ä¹‹é—´çš„ç›¸å…³æ€§ï¼š
```bash
# The closer it's to 1 or -1 the more correlated the data is
# Note that columns are always perfectly correlated with themselves.
dataset[['column_name', 'cloumn_missing_data']].corr()
```
å¦‚æœä½ å†³å®šå¿½ç•¥ç¼ºå¤±çš„æ•°æ®ï¼Œä½ ä»ç„¶éœ€è¦å¤„ç†å®ƒï¼šä½ å¯ä»¥**åˆ é™¤å¸¦æœ‰ç¼ºå¤±æ•°æ®çš„è¡Œ**ï¼ˆæ¨¡å‹çš„è®­ç»ƒæ•°æ®ä¼šå˜å°ï¼‰ï¼Œä½ å¯ä»¥**å®Œå…¨åˆ é™¤è¯¥ç‰¹å¾**ï¼Œæˆ–è€…å¯ä»¥**å¯¹å…¶è¿›è¡Œå»ºæ¨¡**ã€‚

ä½ åº”è¯¥**æ£€æŸ¥ç¼ºå¤±ç‰¹å¾ä¸ç›®æ ‡åˆ—ä¹‹é—´çš„ç›¸å…³æ€§**ï¼Œä»¥äº†è§£è¯¥ç‰¹å¾å¯¹ç›®æ ‡çš„é‡è¦æ€§ï¼Œå¦‚æœå®ƒç¡®å®**å¾ˆå°**ï¼Œä½ å¯ä»¥é€‰æ‹©**åˆ é™¤å®ƒæˆ–å¡«å……å®ƒ**ã€‚

å¯¹äºç¼ºå¤±çš„**è¿ç»­æ•°æ®**ï¼Œä½ å¯ä»¥ä½¿ç”¨ï¼š**å‡å€¼**ã€**ä¸­ä½æ•°**æˆ–ä½¿ç”¨ä¸€ä¸ª**æ’è¡¥ç®—æ³•**ã€‚æ’è¡¥ç®—æ³•å¯ä»¥å°è¯•ä½¿ç”¨å…¶ä»–ç‰¹å¾æ¥æ‰¾åˆ°ç¼ºå¤±ç‰¹å¾çš„å€¼ï¼š
```python
from sklearn.impute import KNNImputer

X = dataset[['column1', 'column2', 'column3']]
y = dataset.column_target

# Create the imputer that will fill the data
imputer = KNNImputer(n_neightbors=2, weights='uniform')
X_imp = imputer.fit_transform(X)

# Check new data
dataset_imp = pd.DataFrame(X_imp)
dataset.columns = ['column1', 'column2', 'column3']
dataset.iloc[10:20] # Get some indexes that contained empty data before
```
ä¸ºäº†å¡«å……åˆ†ç±»æ•°æ®ï¼Œé¦–å…ˆéœ€è¦è€ƒè™‘å€¼ç¼ºå¤±çš„åŸå› ã€‚å¦‚æœæ˜¯ç”±äºç”¨æˆ·çš„é€‰æ‹©ï¼ˆä»–ä»¬ä¸æƒ³æä¾›æ•°æ®ï¼‰ï¼Œå¯ä»¥åˆ›å»ºä¸€ä¸ªæ–°çš„ç±»åˆ«æ¥è¡¨ç¤ºã€‚å¦‚æœæ˜¯ç”±äºäººä¸ºé”™è¯¯ï¼Œå¯ä»¥åˆ é™¤è¡Œæˆ–ç‰¹å¾ï¼ˆè¯·å‚è€ƒå‰é¢æåˆ°çš„æ­¥éª¤ï¼‰ï¼Œæˆ–è€…ç”¨ä¼—æ•°å¡«å……ï¼ˆä¸æ¨èï¼‰ã€‚

# åˆå¹¶ç‰¹å¾

å¦‚æœä½ å‘ç°ä¸¤ä¸ªç‰¹å¾å½¼æ­¤ä¹‹é—´å­˜åœ¨ç›¸å…³æ€§ï¼Œé€šå¸¸åº”è¯¥åˆ é™¤å…¶ä¸­ä¸€ä¸ªï¼ˆä¸ç›®æ ‡ç›¸å…³æ€§è¾ƒä½çš„é‚£ä¸ªï¼‰ï¼Œä½†ä¹Ÿå¯ä»¥å°è¯•å°†å®ƒä»¬åˆå¹¶å¹¶åˆ›å»ºä¸€ä¸ªæ–°çš„ç‰¹å¾ã€‚
```python
# Create a new feautr combining feature1 and feature2
dataset['new_feature'] = dataset.column1/dataset.column2

# Check correlation with target column
dataset[['new_feature', 'column1', 'column2', 'target']].corr()['target'][:]

# Check for collinearity of the 2 features and the new one
X = add_constant(dataset[['column1', 'column2', 'target']])
# Calculate VIF
pd.Series([variance_inflation_factor(X.values, i) for i in range(X.shape[1])], index=X.columns)
```
<details>

<summary><a href="https://cloud.hacktricks.xyz/pentesting-cloud/pentesting-cloud-methodology"><strong>â˜ï¸ HackTricksäº‘ â˜ï¸</strong></a> -<a href="https://twitter.com/hacktricks_live"><strong>ğŸ¦ æ¨ç‰¹ ğŸ¦</strong></a> - <a href="https://www.twitch.tv/hacktricks_live/schedule"><strong>ğŸ™ï¸ Twitch ğŸ™ï¸</strong></a> - <a href="https://www.youtube.com/@hacktricks_LIVE"><strong>ğŸ¥ YouTube ğŸ¥</strong></a></summary>

- ä½ åœ¨ä¸€å®¶**ç½‘ç»œå®‰å…¨å…¬å¸**å·¥ä½œå—ï¼Ÿæƒ³è¦åœ¨HackTricksä¸­**å®£ä¼ ä½ çš„å…¬å¸**å—ï¼Ÿæˆ–è€…ä½ æƒ³è¦**è·å–PEASSçš„æœ€æ–°ç‰ˆæœ¬æˆ–ä¸‹è½½HackTricksçš„PDF**å—ï¼Ÿè¯·æŸ¥çœ‹[**è®¢é˜…è®¡åˆ’**](https://github.com/sponsors/carlospolop)ï¼

- å‘ç°æˆ‘ä»¬çš„ç‹¬å®¶[**NFTs**](https://opensea.io/collection/the-peass-family)æ”¶è—å“â€”â€”[**The PEASS Family**](https://opensea.io/collection/the-peass-family)

- è·å–[**å®˜æ–¹PEASSå’ŒHackTrickså‘¨è¾¹äº§å“**](https://peass.creator-spring.com)

- **åŠ å…¥**[**ğŸ’¬**](https://emojipedia.org/speech-balloon/) [**Discordç¾¤ç»„**](https://discord.gg/hRep4RUj7f)æˆ–[**ç”µæŠ¥ç¾¤ç»„**](https://t.me/peass)ï¼Œæˆ–è€…**å…³æ³¨**æˆ‘åœ¨**Twitter**ä¸Šçš„[**ğŸ¦**](https://github.com/carlospolop/hacktricks/tree/7af18b62b3bdc423e11444677a6a73d4043511e9/\[https:/emojipedia.org/bird/README.md)[**@carlospolopm**](https://twitter.com/hacktricks_live)**ã€‚**

- **é€šè¿‡å‘[hacktricks repo](https://github.com/carlospolop/hacktricks)å’Œ[hacktricks-cloud repo](https://github.com/carlospolop/hacktricks-cloud)æäº¤PRæ¥åˆ†äº«ä½ çš„é»‘å®¢æŠ€å·§**ã€‚

</details>
