<details>

<summary><strong>Aprenda hacking no AWS do zero ao her√≥i com</strong> <a href="https://training.hacktricks.xyz/courses/arte"><strong>htARTE (HackTricks AWS Red Team Expert)</strong></a><strong>!</strong></summary>

Outras formas de apoiar o HackTricks:

* Se voc√™ quer ver sua **empresa anunciada no HackTricks** ou **baixar o HackTricks em PDF**, confira os [**PLANOS DE ASSINATURA**](https://github.com/sponsors/carlospolop)!
* Adquira o [**material oficial PEASS & HackTricks**](https://peass.creator-spring.com)
* Descubra [**A Fam√≠lia PEASS**](https://opensea.io/collection/the-peass-family), nossa cole√ß√£o exclusiva de [**NFTs**](https://opensea.io/collection/the-peass-family)
* **Junte-se ao grupo** üí¨ [**Discord**](https://discord.gg/hRep4RUj7f) ou ao grupo [**telegram**](https://t.me/peass) ou **siga-me** no **Twitter** üê¶ [**@carlospolopm**](https://twitter.com/carlospolopm)**.**
* **Compartilhe suas t√©cnicas de hacking enviando PRs para os reposit√≥rios do GitHub** [**HackTricks**](https://github.com/carlospolop/hacktricks) e [**HackTricks Cloud**](https://github.com/carlospolop/hacktricks-cloud).

</details>


# Tipos b√°sicos de dados poss√≠veis

Os dados podem ser **cont√≠nuos** (**infinitos** valores) ou **categ√≥ricos** (nominais) onde a quantidade de valores poss√≠veis √© **limitada**.

## Tipos Categ√≥ricos

### Bin√°rio

Apenas **2 valores poss√≠veis**: 1 ou 0. No caso de um conjunto de dados onde os valores est√£o em formato de string (por exemplo, "True" e "False"), voc√™ atribui n√∫meros a esses valores com:
```python
dataset["column2"] = dataset.column2.map({"T": 1, "F": 0})
```
### **Ordinal**

Os **valores seguem uma ordem**, como em: 1¬∫ lugar, 2¬∫ lugar... Se as categorias forem strings (como: "iniciante", "amador", "profissional", "especialista"), voc√™ pode mape√°-las para n√∫meros como vimos no caso bin√°rio.
```python
column2_mapping = {'starter':0,'amateur':1,'professional':2,'expert':3}
dataset['column2'] = dataset.column2.map(column2_mapping)
```
* Para **colunas alfab√©ticas** voc√™ pode orden√°-las mais facilmente:
```python
# First get all the uniq values alphabetically sorted
possible_values_sorted = dataset.column2.sort_values().unique().tolist()
# Assign each one a value
possible_values_mapping = {value:idx for idx,value in enumerate(possible_values_sorted)}
dataset['column2'] = dataset.column2.map(possible_values_mapping)
```
### **C√≠clico**

Parece **como valor ordinal** porque h√° uma ordem, mas isso n√£o significa que um √© maior que o outro. Al√©m disso, a **dist√¢ncia entre eles depende da dire√ß√£o** que voc√™ est√° contando. Exemplo: Os dias da semana, domingo n√£o √© "maior" que segunda-feira.

* Existem **diferentes maneiras** de codificar caracter√≠sticas c√≠clicas, algumas podem funcionar apenas com alguns algoritmos. **Em geral, a codifica√ß√£o dummy pode ser usada**
```python
column2_dummies = pd.get_dummies(dataset.column2, drop_first=True)
dataset_joined = pd.concat([dataset[['column2']], column2_dummies], axis=1)
```
### **Datas**

Datas s√£o **vari√°veis cont√≠nuas**. Podem ser vistas como **c√≠clicas** (porque se repetem) **ou** como **vari√°veis ordinais** (porque um momento √© maior que o anterior).

* Geralmente, datas s√£o usadas como **√≠ndice**
```python
# Transform dates to datetime
dataset["column_date"] = pd.to_datetime(dataset.column_date)
# Make the date feature the index
dataset.set_index('column_date', inplace=True)
print(dataset.head())

# Sum usage column per day
daily_sum = dataset.groupby(df_daily_usage.index.date).agg({'usage':['sum']})
# Flatten and rename usage column
daily_sum.columns = daily_sum.columns.get_level_values(0)
daily_sum.columns = ['daily_usage']
print(daily_sum.head())

# Fill days with 0 usage
idx = pd.date_range('2020-01-01', '2020-12-31')
daily_sum.index = pd.DatetimeIndex(daily_sum.index)
df_filled = daily_sum.reindex(idx, fill_value=0) # Fill missing values


# Get day of the week, Monday=0, Sunday=6, and week days names
dataset['DoW'] = dataset.transaction_date.dt.dayofweek
# do the same in a different way
dataset['weekday'] = dataset.transaction_date.dt.weekday
# get day names
dataset['day_name'] = dataset.transaction_date.apply(lambda x: x.day_name())
```
### Multi-categoria/nominal

**Mais de 2 categorias** sem ordem relacionada. Use `dataset.describe(include='all')` para obter informa√ß√µes sobre as categorias de cada recurso.

* Uma **string de refer√™ncia** √© uma **coluna que identifica um exemplo** (como o nome de uma pessoa). Isso pode ser duplicado (porque 2 pessoas podem ter o mesmo nome), mas a maioria ser√° √∫nica. Esses dados s√£o **in√∫teis e devem ser removidos**.
* Uma **coluna chave** √© usada para **vincular dados entre tabelas**. Neste caso, os elementos s√£o √∫nicos. Esses dados s√£o **in√∫teis e devem ser removidos**.

Para **codificar colunas de multi-categoria em n√∫meros** (para que o algoritmo de ML as entenda), **codifica√ß√£o dummy √© usada** (e **n√£o codifica√ß√£o one-hot** porque n√£o **evita multicolinearidade perfeita**).

Voc√™ pode obter uma **coluna de multi-categoria codificada em one-hot** com `pd.get_dummies(dataset.column1)`. Isso transformar√° todas as classes em recursos bin√°rios, criando assim **uma nova coluna por classe poss√≠vel** e atribuir√° 1 **valor Verdadeiro a uma coluna**, e o restante ser√° falso.

Voc√™ pode obter uma **coluna de multi-categoria codificada em dummy** com `pd.get_dummies(dataset.column1, drop_first=True)`. Isso transformar√° todas as classes em recursos bin√°rios, criando assim **uma nova coluna por classe poss√≠vel menos uma**, j√° que **as √∫ltimas 2 colunas ser√£o refletidas como "1" ou "0" na √∫ltima coluna bin√°ria criada**. Isso evitar√° multicolinearidade perfeita, reduzindo as rela√ß√µes entre colunas.

# Colinear/Multicolinearidade

Colinear aparece quando **2 recursos est√£o relacionados um com o outro**. Multicolinearidade aparece quando s√£o mais de 2.

Em ML **voc√™ quer que seus recursos estejam relacionados com os resultados poss√≠veis, mas voc√™ n√£o quer que eles estejam relacionados entre si**. √â por isso que a **codifica√ß√£o dummy mistura as duas √∫ltimas colunas** disso e **√© melhor do que a codifica√ß√£o one-hot**, que n√£o faz isso, criando uma rela√ß√£o clara entre todos os novos recursos da coluna de multi-categoria.

VIF √© o **Fator de Infla√ß√£o de Vari√¢ncia** que **mede a multicolinearidade dos recursos**. Um valor **acima de 5 significa que um dos dois ou mais recursos colineares deve ser removido**.
```python
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

#dummies_encoded = pd.get_dummies(dataset.column1, drop_first=True)
onehot_encoded = pd.get_dummies(dataset.column1)
X = add_constant(onehot_encoded) # Add previously one-hot encoded data
print(pd.Series([variance_inflation_factor(X.values,i) for i in range(X.shape[1])], index=X.columns))
```
# Desequil√≠brio Categ√≥rico

Isso ocorre quando **n√£o h√° a mesma quantidade de cada categoria** nos dados de treinamento.
```python
# Get statistic of the features
print(dataset.describe(include='all'))
# Get an overview of the features
print(dataset.info())
# Get imbalance information of the target column
print(dataset.target_column.value_counts())
```
Em um desequil√≠brio, sempre h√° uma **classe ou classes majorit√°rias** e uma **classe ou classes minorit√°rias**.

Existem 2 maneiras principais de corrigir esse problema:

* **Undersampling**: Remo√ß√£o aleat√≥ria de dados da classe majorit√°ria para que ela tenha o mesmo n√∫mero de amostras que a classe minorit√°ria.
```python
from imblearn.under_sampling import RandomUnderSampler
rus = RandomUserSampler(random_state=1337)

X = dataset[['column1', 'column2', 'column3']].copy()
y = dataset.target_column

X_under, y_under = rus.fit_resample(X,y)
print(y_under.value_counts()) #Confirm data isn't imbalanced anymore
```
* **Oversampling**: Gerar mais dados para a classe minorit√°ria at√© que ela tenha tantas amostras quanto a classe majorit√°ria.
```python
from imblearn.under_sampling import RandomOverSampler
ros = RandomOverSampler(random_state=1337)

X = dataset[['column1', 'column2', 'column3']].copy()
y = dataset.target_column

X_over, y_over = ros.fit_resample(X,y)
print(y_over.value_counts()) #Confirm data isn't imbalanced anymore
```
Voc√™ pode usar o argumento **`sampling_strategy`** para indicar a **porcentagem** que deseja **subamostrar ou sobreamostrar** (**por padr√£o √© 1 (100%)**, o que significa igualar o n√∫mero de classes minorit√°rias com as classes majorit√°rias)

{% hint style="info" %}
Subamostragem ou Sobreamostragem n√£o s√£o perfeitas se voc√™ obter estat√≠sticas (com `.describe()`) dos dados sub/sobreamostrados e compar√°-los com os originais, voc√™ ver√° **que eles mudaram.** Portanto, sobreamostragem e subamostragem est√£o modificando os dados de treinamento.
{% endhint %}

## Sobreamostragem com SMOTE

**SMOTE** geralmente √© uma **maneira mais confi√°vel de sobreamostrar os dados**.
```python
from imblearn.over_sampling import SMOTE

# Form SMOTE the target_column need to be numeric, map it if necessary
smote = SMOTE(random_state=1337)
X_smote, y_smote = smote.fit_resample(dataset[['column1', 'column2', 'column3']], dataset.target_column)
dataset_smote = pd.DataFrame(X_smote, columns=['column1', 'column2', 'column3'])
dataset['target_column'] = y_smote
print(y_smote.value_counts()) #Confirm data isn't imbalanced anymore
```
# Categorias Raramente Ocorrentes

Imagine um conjunto de dados onde uma das classes alvo **ocorre muito poucas vezes**.

Isso √© como o desequil√≠brio de categoria da se√ß√£o anterior, mas a categoria raramente ocorrente est√° ocorrendo ainda menos que a "classe minorit√°ria" naquele caso. Os m√©todos de **oversampling** e **undersampling** **puros** tamb√©m podem ser usados aqui, mas geralmente essas t√©cnicas **n√£o fornecer√£o resultados realmente bons**.

## Pesos

Em alguns algoritmos √© poss√≠vel **modificar os pesos dos dados alvo** para que alguns deles tenham por padr√£o mais import√¢ncia ao gerar o modelo.
```python
weights = {0: 10 1:1} #Assign weight 10 to False and 1 to True
model = LogisticRegression(class_weight=weights)
```
Voc√™ pode **misturar os pesos com t√©cnicas de sobreamostragem/subamostragem** para tentar melhorar os resultados.

## PCA - An√°lise de Componentes Principais

√â um m√©todo que ajuda a reduzir a dimensionalidade dos dados. Vai **combinar diferentes caracter√≠sticas** para **reduzir a quantidade** delas, gerando **caracter√≠sticas mais √∫teis** (_menos computa√ß√£o √© necess√°ria_).

As caracter√≠sticas resultantes n√£o s√£o compreens√≠veis por humanos, ent√£o tamb√©m **anonimiza os dados**.

# Categorias de R√≥tulos Incongruentes

Os dados podem ter erros devido a transforma√ß√µes mal-sucedidas ou simplesmente por erro humano ao escrever os dados.

Portanto, voc√™ pode encontrar o **mesmo r√≥tulo com erros de ortografia**, diferentes **capitaliza√ß√µes**, **abrevia√ß√µes** como: _BLUE, Blue, b, bule_. Voc√™ precisa corrigir esses erros de r√≥tulo dentro dos dados antes de treinar o modelo.

Voc√™ pode limpar esses problemas tornando tudo min√∫sculo e mapeando r√≥tulos mal escritos para os corretos.

√â muito importante verificar que **todos os dados que voc√™ possui est√£o corretamente rotulados**, porque, por exemplo, um erro de ortografia nos dados, ao codificar as classes em dummies, gerar√° uma nova coluna nas caracter√≠sticas finais com **consequ√™ncias ruins para o modelo final**. Este exemplo pode ser detectado facilmente codificando uma coluna em one-hot e verificando os nomes das colunas criadas.

# Dados Ausentes

Alguns dados do estudo podem estar ausentes.

Pode acontecer de alguns dados completamente aleat√≥rios estarem ausentes por algum erro. Esse tipo de dado √© **Missing Completely at Random** (**MCAR**).

Pode ser que alguns dados aleat√≥rios estejam ausentes, mas h√° algo que torna mais prov√°vel que alguns detalhes espec√≠ficos estejam ausentes, por exemplo, √© mais frequente que homens revelem sua idade, mas n√£o mulheres. Isso √© chamado **Missing at Random** (**MAR**).

Finalmente, pode haver dados **Missing Not at Random** (**MNAR**). O valor dos dados est√° diretamente relacionado com a probabilidade de ter os dados. Por exemplo, se voc√™ quer medir algo embara√ßoso, quanto mais embara√ßosa for a pessoa, menos prov√°vel √© que ela compartilhe isso.

As **duas primeiras categorias** de dados ausentes podem ser **ignor√°veis**. Mas a **terceira** requer considerar **apenas partes dos dados** que n√£o s√£o impactadas ou tentar **modelar os dados ausentes de alguma forma**.

Uma maneira de descobrir sobre dados ausentes √© usar a fun√ß√£o `.info()`, pois ela indicar√° o **n√∫mero de linhas, mas tamb√©m o n√∫mero de valores por categoria**. Se alguma categoria tem menos valores do que o n√∫mero de linhas, ent√£o h√° alguns dados ausentes:
```bash
# Get info of the dataset
dataset.info()

# Drop all rows where some value is missing
dataset.dropna(how='any', axis=0).info()
```
Geralmente √© recomendado que se um recurso estiver **ausente em mais de 20%** do conjunto de dados, a **coluna deve ser removida:**
```bash
# Remove column
dataset.drop('Column_name', axis='columns', inplace=True)
dataset.info()
```
{% hint style="info" %}
Observe que **nem todos os valores ausentes est√£o faltando no conjunto de dados**. √â poss√≠vel que valores ausentes tenham sido atribu√≠dos o valor "Desconhecido", "n/a", "", -1, 0... Voc√™ precisa verificar o conjunto de dados (usando `dataset.column`_`name.value`_`counts(dropna=False)` para verificar os poss√≠veis valores).
{% endhint %}

Se alguns dados est√£o ausentes no conjunto de dados (e n√£o s√£o muitos), voc√™ precisa encontrar a **categoria dos dados ausentes**. Para isso, basicamente precisa saber se os **dados ausentes s√£o aleat√≥rios ou n√£o**, e para isso voc√™ precisa descobrir se os **dados ausentes estavam correlacionados com outros dados** do conjunto de dados.

Para descobrir se um valor ausente est√° correlacionado com outra coluna, voc√™ pode criar uma nova coluna que coloque 1s e 0s se o dado est√° ausente ou n√£o e ent√£o calcular a correla√ß√£o entre eles:
```bash
# The closer it's to 1 or -1 the more correlated the data is
# Note that columns are always perfectly correlated with themselves.
dataset[['column_name', 'cloumn_missing_data']].corr()
```
Se voc√™ decidir ignorar os dados ausentes, ainda precisar√° decidir o que fazer com eles: Voc√™ pode **remover as linhas** com dados ausentes (os dados de treino para o modelo ser√£o menores), pode **remover a caracter√≠stica** completamente, ou poderia **model√°-la**.

Voc√™ deve **verificar a correla√ß√£o entre a caracter√≠stica ausente com a coluna alvo** para ver qu√£o importante essa caracter√≠stica √© para o alvo, se for realmente **pequena** voc√™ pode **descart√°-la ou preench√™-la**.

Para preencher dados **cont√≠nuos** ausentes, voc√™ poderia usar: a **m√©dia**, a **mediana** ou usar um algoritmo de **imputa√ß√£o**. O algoritmo de imputa√ß√£o pode tentar usar outras caracter√≠sticas para encontrar um valor para a caracter√≠stica ausente:
```python
from sklearn.impute import KNNImputer

X = dataset[['column1', 'column2', 'column3']]
y = dataset.column_target

# Create the imputer that will fill the data
imputer = KNNImputer(n_neightbors=2, weights='uniform')
X_imp = imputer.fit_transform(X)

# Check new data
dataset_imp = pd.DataFrame(X_imp)
dataset.columns = ['column1', 'column2', 'column3']
dataset.iloc[10:20] # Get some indexes that contained empty data before
```
Para preencher dados categ√≥ricos, primeiro voc√™ precisa pensar se h√° algum motivo pelo qual os valores est√£o ausentes. Se for por **escolha dos usu√°rios** (eles n√£o quiseram fornecer os dados), talvez voc√™ possa **criar uma nova categoria** indicando isso. Se for devido a erro humano, voc√™ pode **remover as linhas** ou a **caracter√≠stica** (verifique os passos mencionados antes) ou **preench√™-la com a moda, a categoria mais usada** (n√£o recomendado).

# Combinando Caracter√≠sticas

Se voc√™ encontrar **duas caracter√≠sticas** que est√£o **correlacionadas** entre si, geralmente voc√™ deve **descartar** uma delas (aquela que √© menos correlacionada com o alvo), mas voc√™ tamb√©m pode tentar **combin√°-las e criar uma nova caracter√≠stica**.
```python
# Create a new feautr combining feature1 and feature2
dataset['new_feature'] = dataset.column1/dataset.column2

# Check correlation with target column
dataset[['new_feature', 'column1', 'column2', 'target']].corr()['target'][:]

# Check for collinearity of the 2 features and the new one
X = add_constant(dataset[['column1', 'column2', 'target']])
# Calculate VIF
pd.Series([variance_inflation_factor(X.values, i) for i in range(X.shape[1])], index=X.columns)
```
<details>

<summary><strong>Aprenda hacking no AWS do zero ao her√≥i com</strong> <a href="https://training.hacktricks.xyz/courses/arte"><strong>htARTE (HackTricks AWS Red Team Expert)</strong></a><strong>!</strong></summary>

Outras formas de apoiar o HackTricks:

* Se voc√™ quer ver sua **empresa anunciada no HackTricks** ou **baixar o HackTricks em PDF**, confira os [**PLANOS DE ASSINATURA**](https://github.com/sponsors/carlospolop)!
* Adquira o [**material oficial PEASS & HackTricks**](https://peass.creator-spring.com)
* Descubra [**A Fam√≠lia PEASS**](https://opensea.io/collection/the-peass-family), nossa cole√ß√£o de [**NFTs**](https://opensea.io/collection/the-peass-family) exclusivos
* **Junte-se ao grupo** üí¨ [**Discord**](https://discord.gg/hRep4RUj7f) ou ao grupo [**telegram**](https://t.me/peass) ou **siga-me** no **Twitter** üê¶ [**@carlospolopm**](https://twitter.com/carlospolopm)**.**
* **Compartilhe suas t√©cnicas de hacking enviando PRs para os reposit√≥rios github do** [**HackTricks**](https://github.com/carlospolop/hacktricks) e [**HackTricks Cloud**](https://github.com/carlospolop/hacktricks-cloud).

</details>
