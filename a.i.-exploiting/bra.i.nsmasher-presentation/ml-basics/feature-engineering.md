<details>

<summary><strong>é€šè¿‡</strong> <a href="https://training.hacktricks.xyz/courses/arte"><strong>htARTE (HackTricks AWS çº¢é˜Ÿä¸“å®¶)</strong></a><strong>ä»é›¶åˆ°è‹±é›„å­¦ä¹  AWS é»‘å®¢æ”»å‡»ï¼</strong></summary>

å…¶ä»–æ”¯æŒ HackTricks çš„æ–¹å¼ï¼š

* å¦‚æœæ‚¨æƒ³åœ¨ **HackTricks ä¸­çœ‹åˆ°æ‚¨çš„å…¬å¸å¹¿å‘Š** æˆ– **ä¸‹è½½ HackTricks çš„ PDF**ï¼Œè¯·æŸ¥çœ‹ [**è®¢é˜…è®¡åˆ’**](https://github.com/sponsors/carlospolop)ï¼
* è·å– [**å®˜æ–¹ PEASS & HackTricks å•†å“**](https://peass.creator-spring.com)
* å‘ç° [**PEASS å®¶æ—**](https://opensea.io/collection/the-peass-family)ï¼Œæˆ‘ä»¬ç‹¬å®¶çš„ [**NFT é›†åˆ**](https://opensea.io/collection/the-peass-family)
* **åŠ å…¥** ğŸ’¬ [**Discord ç¾¤ç»„**](https://discord.gg/hRep4RUj7f) æˆ– [**telegram ç¾¤ç»„**](https://t.me/peass) æˆ–åœ¨ **Twitter** ğŸ¦ ä¸Š **å…³æ³¨** æˆ‘ [**@carlospolopm**](https://twitter.com/carlospolopm)**ã€‚**
* **é€šè¿‡å‘** [**HackTricks**](https://github.com/carlospolop/hacktricks) å’Œ [**HackTricks Cloud**](https://github.com/carlospolop/hacktricks-cloud) github ä»“åº“æäº¤ PR æ¥åˆ†äº«æ‚¨çš„é»‘å®¢æŠ€å·§ã€‚

</details>


# å¯èƒ½æ•°æ®çš„åŸºæœ¬ç±»å‹

æ•°æ®å¯ä»¥æ˜¯ **è¿ç»­çš„**ï¼ˆ**æ— é™** å€¼ï¼‰æˆ– **åˆ†ç±»çš„**ï¼ˆåä¹‰çš„ï¼‰ï¼Œå…¶ä¸­å¯èƒ½çš„å€¼æ•°é‡æ˜¯ **æœ‰é™çš„**ã€‚

## åˆ†ç±»ç±»å‹

### äºŒè¿›åˆ¶

åªæœ‰ **2ä¸ªå¯èƒ½çš„å€¼**ï¼š1 æˆ– 0ã€‚å¦‚æœåœ¨æ•°æ®é›†ä¸­å€¼æ˜¯å­—ç¬¦ä¸²æ ¼å¼ï¼ˆä¾‹å¦‚ "True" å’Œ "False"ï¼‰ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ–¹æ³•ä¸ºè¿™äº›å€¼åˆ†é…æ•°å­—ï¼š
```python
dataset["column2"] = dataset.column2.map({"T": 1, "F": 0})
```
### **åºæ•°**

**å€¼éµå¾ªä¸€ä¸ªé¡ºåº**ï¼Œä¾‹å¦‚ï¼šç¬¬1åã€ç¬¬2åâ€¦â€¦å¦‚æœç±»åˆ«æ˜¯å­—ç¬¦ä¸²ï¼ˆå¦‚ï¼šâ€œæ–°æ‰‹â€ã€â€œä¸šä½™â€ã€â€œä¸“ä¸šâ€ã€â€œä¸“å®¶â€ï¼‰ï¼Œæ‚¨å¯ä»¥åƒæˆ‘ä»¬åœ¨äºŒè¿›åˆ¶æ¡ˆä¾‹ä¸­çœ‹åˆ°çš„é‚£æ ·å°†å®ƒä»¬æ˜ å°„åˆ°æ•°å­—ã€‚
```python
column2_mapping = {'starter':0,'amateur':1,'professional':2,'expert':3}
dataset['column2'] = dataset.column2.map(column2_mapping)
```
* å¯¹äº**å­—æ¯åˆ—**ï¼Œæ‚¨å¯ä»¥æ›´å®¹æ˜“åœ°å¯¹å®ƒä»¬è¿›è¡Œæ’åºï¼š
```python
# First get all the uniq values alphabetically sorted
possible_values_sorted = dataset.column2.sort_values().unique().tolist()
# Assign each one a value
possible_values_mapping = {value:idx for idx,value in enumerate(possible_values_sorted)}
dataset['column2'] = dataset.column2.map(possible_values_mapping)
```
### **å‘¨æœŸæ€§**

çœ‹èµ·æ¥**åƒåºæ•°å€¼**å› ä¸ºæœ‰ä¸€ä¸ªé¡ºåºï¼Œä½†è¿™å¹¶ä¸æ„å‘³ç€ä¸€ä¸ªæ¯”å¦ä¸€ä¸ªå¤§ã€‚åŒæ ·ï¼Œå®ƒä»¬ä¹‹é—´çš„**è·ç¦»å–å†³äºä½ è®¡æ•°çš„æ–¹å‘**ã€‚ä¾‹å¦‚ï¼šä¸€å‘¨çš„æ—¥å­ï¼Œæ˜ŸæœŸæ—¥å¹¶ä¸æ¯”æ˜ŸæœŸä¸€â€œå¤§â€ã€‚

* æœ‰**ä¸åŒçš„æ–¹æ³•**æ¥ç¼–ç å‘¨æœŸæ€§ç‰¹å¾ï¼Œæœ‰äº›æ–¹æ³•å¯èƒ½åªé€‚ç”¨äºæŸäº›ç®—æ³•ã€‚**é€šå¸¸ï¼Œå¯ä»¥ä½¿ç”¨è™šæ‹Ÿç¼–ç **
```python
column2_dummies = pd.get_dummies(dataset.column2, drop_first=True)
dataset_joined = pd.concat([dataset[['column2']], column2_dummies], axis=1)
```
### **æ—¥æœŸ**

æ—¥æœŸæ˜¯**è¿ç»­çš„** **å˜é‡**ã€‚å¯ä»¥è¢«è§†ä¸º**å‘¨æœŸæ€§çš„**ï¼ˆå› ä¸ºå®ƒä»¬ä¼šé‡å¤ï¼‰**æˆ–è€…**æ˜¯**åºæ•°**å˜é‡ï¼ˆå› ä¸ºä¸€ä¸ªæ—¶é—´ç‚¹æ¯”ä¹‹å‰çš„æ—¶é—´ç‚¹å¤§ï¼‰ã€‚

* é€šå¸¸æ—¥æœŸè¢«ç”¨ä½œ**ç´¢å¼•**
```python
# Transform dates to datetime
dataset["column_date"] = pd.to_datetime(dataset.column_date)
# Make the date feature the index
dataset.set_index('column_date', inplace=True)
print(dataset.head())

# Sum usage column per day
daily_sum = dataset.groupby(df_daily_usage.index.date).agg({'usage':['sum']})
# Flatten and rename usage column
daily_sum.columns = daily_sum.columns.get_level_values(0)
daily_sum.columns = ['daily_usage']
print(daily_sum.head())

# Fill days with 0 usage
idx = pd.date_range('2020-01-01', '2020-12-31')
daily_sum.index = pd.DatetimeIndex(daily_sum.index)
df_filled = daily_sum.reindex(idx, fill_value=0) # Fill missing values


# Get day of the week, Monday=0, Sunday=6, and week days names
dataset['DoW'] = dataset.transaction_date.dt.dayofweek
# do the same in a different way
dataset['weekday'] = dataset.transaction_date.dt.weekday
# get day names
dataset['day_name'] = dataset.transaction_date.apply(lambda x: x.day_name())
```
### å¤šç±»åˆ«/åä¹‰

**è¶…è¿‡2ä¸ªç±»åˆ«**ï¼Œæ²¡æœ‰ç›¸å…³é¡ºåºã€‚ä½¿ç”¨ `dataset.describe(include='all')` æ¥è·å–æ¯ä¸ªç‰¹å¾çš„ç±»åˆ«ä¿¡æ¯ã€‚

* **å¼•ç”¨å­—ç¬¦ä¸²**æ˜¯**æ ‡è¯†ç¤ºä¾‹çš„åˆ—**ï¼ˆå¦‚ä¸€ä¸ªäººçš„åå­—ï¼‰ã€‚è¿™å¯èƒ½ä¼šé‡å¤ï¼ˆå› ä¸ºä¸¤ä¸ªäººå¯èƒ½æœ‰ç›¸åŒçš„åå­—ï¼‰ï¼Œä½†å¤§å¤šæ•°å°†æ˜¯å”¯ä¸€çš„ã€‚è¿™äº›æ•°æ®**æ— ç”¨ä¸”åº”è¯¥è¢«ç§»é™¤**ã€‚
* **å…³é”®åˆ—**ç”¨äº**é“¾æ¥è¡¨ä¹‹é—´çš„æ•°æ®**ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå…ƒç´ æ˜¯å”¯ä¸€çš„ã€‚è¿™äº›æ•°æ®**æ— ç”¨ä¸”åº”è¯¥è¢«ç§»é™¤**ã€‚

è¦**å°†å¤šç±»åˆ«åˆ—ç¼–ç ä¸ºæ•°å­—**ï¼ˆä»¥ä¾¿æœºå™¨å­¦ä¹ ç®—æ³•ç†è§£å®ƒä»¬ï¼‰ï¼Œ**ä½¿ç”¨è™šæ‹Ÿç¼–ç **ï¼ˆè€Œ**ä¸æ˜¯ç‹¬çƒ­ç¼–ç **ï¼Œå› ä¸ºå®ƒ**ä¸èƒ½é¿å…å®Œå…¨å¤šé‡å…±çº¿æ€§**ï¼‰ã€‚

æ‚¨å¯ä»¥ä½¿ç”¨ `pd.get_dummies(dataset.column1)` è·å¾—**ç‹¬çƒ­ç¼–ç çš„å¤šç±»åˆ«åˆ—**ã€‚è¿™å°†æŠŠæ‰€æœ‰ç±»åˆ«è½¬æ¢ä¸ºäºŒè¿›åˆ¶ç‰¹å¾ï¼Œå› æ­¤è¿™å°†ä¸º**æ¯ä¸ªå¯èƒ½çš„ç±»åˆ«åˆ›å»ºä¸€ä¸ªæ–°åˆ—**ï¼Œå¹¶ä¸ºä¸€ä¸ªåˆ—åˆ†é…1ä¸ª**Trueå€¼**ï¼Œå…¶ä½™çš„å°†æ˜¯å‡çš„ã€‚

æ‚¨å¯ä»¥ä½¿ç”¨ `pd.get_dummies(dataset.column1, drop_first=True)` è·å¾—**è™šæ‹Ÿç¼–ç çš„å¤šç±»åˆ«åˆ—**ã€‚è¿™å°†æŠŠæ‰€æœ‰ç±»åˆ«è½¬æ¢ä¸ºäºŒè¿›åˆ¶ç‰¹å¾ï¼Œå› æ­¤è¿™å°†ä¸º**æ¯ä¸ªå¯èƒ½çš„ç±»åˆ«åˆ›å»ºä¸€ä¸ªæ–°åˆ—å‡å»ä¸€ä¸ª**ï¼Œå› ä¸º**æœ€åä¸¤åˆ—å°†åœ¨æœ€ååˆ›å»ºçš„äºŒè¿›åˆ¶åˆ—ä¸­åæ˜ ä¸ºâ€œ1â€æˆ–â€œ0â€**ã€‚è¿™å°†é¿å…å®Œå…¨å¤šé‡å…±çº¿æ€§ï¼Œå‡å°‘åˆ—ä¹‹é—´çš„å…³ç³»ã€‚

# å…±çº¿/å¤šé‡å…±çº¿æ€§

å½“**ä¸¤ä¸ªç‰¹å¾å½¼æ­¤ç›¸å…³**æ—¶ï¼Œä¼šå‡ºç°å…±çº¿ã€‚å½“è¿™äº›ç‰¹å¾è¶…è¿‡2ä¸ªæ—¶ï¼Œä¼šå‡ºç°å¤šé‡å…±çº¿æ€§ã€‚

åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œ**æ‚¨å¸Œæœ›æ‚¨çš„ç‰¹å¾ä¸å¯èƒ½çš„ç»“æœç›¸å…³ï¼Œä½†æ‚¨ä¸å¸Œæœ›å®ƒä»¬ä¹‹é—´ç›¸äº’ç›¸å…³**ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆ**è™šæ‹Ÿç¼–ç æ··åˆäº†æœ€åä¸¤åˆ—**ï¼Œå¹¶ä¸”**æ¯”ç‹¬çƒ­ç¼–ç æ›´å¥½**ï¼Œåè€…ä¸è¿™æ ·åšï¼Œä»è€Œåœ¨å¤šç±»åˆ«åˆ—çš„æ‰€æœ‰æ–°ç‰¹å¾ä¹‹é—´åˆ›å»ºäº†æ˜æ˜¾çš„å…³ç³»ã€‚

VIFæ˜¯**æ–¹å·®è†¨èƒ€å› å­**ï¼Œå®ƒ**è¡¡é‡ç‰¹å¾çš„å¤šé‡å…±çº¿æ€§**ã€‚ä¸€ä¸ª**é«˜äº5çš„å€¼æ„å‘³ç€åº”è¯¥ç§»é™¤ä¸¤ä¸ªæˆ–æ›´å¤šå…±çº¿ç‰¹å¾ä¸­çš„ä¸€ä¸ª**ã€‚
```python
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

#dummies_encoded = pd.get_dummies(dataset.column1, drop_first=True)
onehot_encoded = pd.get_dummies(dataset.column1)
X = add_constant(onehot_encoded) # Add previously one-hot encoded data
print(pd.Series([variance_inflation_factor(X.values,i) for i in range(X.shape[1])], index=X.columns))
```
# ç±»åˆ«ä¸å¹³è¡¡

è¿™å‘ç”Ÿåœ¨è®­ç»ƒæ•°æ®ä¸­**æ¯ä¸ªç±»åˆ«çš„æ•°é‡ä¸ç›¸åŒ**æ—¶ã€‚
```python
# Get statistic of the features
print(dataset.describe(include='all'))
# Get an overview of the features
print(dataset.info())
# Get imbalance information of the target column
print(dataset.target_column.value_counts())
```
åœ¨ä¸å¹³è¡¡ä¸­ï¼Œæ€»æ˜¯å­˜åœ¨**å¤šæ•°ç±»åˆ«**å’Œ**å°‘æ•°ç±»åˆ«**ã€‚

æœ‰ä¸¤ç§ä¸»è¦æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼š

* **æ¬ é‡‡æ ·**ï¼šéšæœºç§»é™¤å¤šæ•°ç±»åˆ«ä¸­çš„æ•°æ®ï¼Œä½¿å…¶æ ·æœ¬æ•°é‡ä¸å°‘æ•°ç±»åˆ«ç›¸åŒã€‚
```python
from imblearn.under_sampling import RandomUnderSampler
rus = RandomUserSampler(random_state=1337)

X = dataset[['column1', 'column2', 'column3']].copy()
y = dataset.target_column

X_under, y_under = rus.fit_resample(X,y)
print(y_under.value_counts()) #Confirm data isn't imbalanced anymore
```
* **Oversampling**ï¼šä¸ºå°‘æ•°ç±»ç”Ÿæˆæ›´å¤šæ•°æ®ï¼Œç›´åˆ°å…¶æ ·æœ¬æ•°é‡ä¸å¤šæ•°ç±»ç›¸åŒã€‚
```python
from imblearn.under_sampling import RandomOverSampler
ros = RandomOverSampler(random_state=1337)

X = dataset[['column1', 'column2', 'column3']].copy()
y = dataset.target_column

X_over, y_over = ros.fit_resample(X,y)
print(y_over.value_counts()) #Confirm data isn't imbalanced anymore
```
æ‚¨å¯ä»¥ä½¿ç”¨å‚æ•° **`sampling_strategy`** æ¥æŒ‡å®šæ‚¨æƒ³è¦è¿›è¡Œ**ä¸‹é‡‡æ ·æˆ–è¿‡é‡‡æ ·**çš„**ç™¾åˆ†æ¯”**ï¼ˆ**é»˜è®¤å€¼æ˜¯ 1ï¼ˆ100%ï¼‰**ï¼Œæ„å‘³ç€ä½¿å°‘æ•°ç±»çš„æ•°é‡ä¸å¤šæ•°ç±»çš„æ•°é‡ç›¸ç­‰ï¼‰

{% hint style="info" %}
ä¸‹é‡‡æ ·æˆ–è¿‡é‡‡æ ·å¹¶ä¸å®Œç¾ï¼Œå¦‚æœæ‚¨è·å–è¿‡é‡‡æ ·æˆ–ä¸‹é‡‡æ ·æ•°æ®çš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆä½¿ç”¨ `.describe()`ï¼‰å¹¶å°†å…¶ä¸åŸå§‹æ•°æ®è¿›è¡Œæ¯”è¾ƒï¼Œæ‚¨ä¼šçœ‹åˆ°**å®ƒä»¬å‘ç”Ÿäº†å˜åŒ–**ã€‚å› æ­¤ï¼Œè¿‡é‡‡æ ·å’Œä¸‹é‡‡æ ·æ­£åœ¨ä¿®æ”¹è®­ç»ƒæ•°æ®ã€‚
{% endhint %}

## SMOTE è¿‡é‡‡æ ·

**SMOTE** é€šå¸¸æ˜¯ä¸€ç§**æ›´å¯é çš„è¿‡é‡‡æ ·æ•°æ®æ–¹å¼**ã€‚
```python
from imblearn.over_sampling import SMOTE

# Form SMOTE the target_column need to be numeric, map it if necessary
smote = SMOTE(random_state=1337)
X_smote, y_smote = smote.fit_resample(dataset[['column1', 'column2', 'column3']], dataset.target_column)
dataset_smote = pd.DataFrame(X_smote, columns=['column1', 'column2', 'column3'])
dataset['target_column'] = y_smote
print(y_smote.value_counts()) #Confirm data isn't imbalanced anymore
```
# ç½•è§ç±»åˆ«

æƒ³è±¡ä¸€ä¸ªæ•°æ®é›†ï¼Œå…¶ä¸­ä¸€ä¸ªç›®æ ‡ç±»åˆ«**å‡ºç°çš„æ¬¡æ•°éå¸¸å°‘**ã€‚

è¿™ç±»ä¼¼äºä¸Šä¸€èŠ‚ä¸­çš„ç±»åˆ«ä¸å¹³è¡¡ï¼Œä½†ç½•è§ç±»åˆ«çš„å‡ºç°æ¬¡æ•°ç”šè‡³æ¯”é‚£ç§æƒ…å†µä¸‹çš„â€œå°‘æ•°ç±»â€è¿˜è¦å°‘ã€‚**åŸå§‹**çš„**è¿‡é‡‡æ ·**å’Œ**æ¬ é‡‡æ ·**æ–¹æ³•ä¹Ÿå¯ä»¥åœ¨è¿™é‡Œä½¿ç”¨ï¼Œä½†é€šå¸¸è¿™äº›æŠ€æœ¯**ä¸ä¼šç»™å‡ºçœŸæ­£å¥½çš„ç»“æœ**ã€‚

## æƒé‡

åœ¨æŸäº›ç®—æ³•ä¸­ï¼Œå¯ä»¥**ä¿®æ”¹ç›®æ ‡æ•°æ®çš„æƒé‡**ï¼Œå› æ­¤åœ¨ç”Ÿæˆæ¨¡å‹æ—¶ï¼Œé»˜è®¤æƒ…å†µä¸‹æŸäº›æ•°æ®ä¼šæ›´é‡è¦ã€‚
```python
weights = {0: 10 1:1} #Assign weight 10 to False and 1 to True
model = LogisticRegression(class_weight=weights)
```
ä½ å¯ä»¥**å°†æƒé‡ä¸è¿‡é‡‡æ ·/æ¬ é‡‡æ ·æŠ€æœ¯æ··åˆ**ï¼Œå°è¯•æ”¹å–„ç»“æœã€‚

## PCA - ä¸»æˆåˆ†åˆ†æ

è¿™æ˜¯ä¸€ç§å¸®åŠ©é™ä½æ•°æ®ç»´åº¦çš„æ–¹æ³•ã€‚å®ƒå°†**ç»“åˆä¸åŒç‰¹å¾**ä»¥**å‡å°‘**å®ƒä»¬çš„æ•°é‡ï¼Œç”Ÿæˆ**æ›´æœ‰ç”¨çš„ç‰¹å¾**ï¼ˆ_å‡å°‘äº†è®¡ç®—éœ€æ±‚_ï¼‰ã€‚

ç»“æœç‰¹å¾å¯¹äººç±»æ¥è¯´æ˜¯ä¸å¯ç†è§£çš„ï¼Œå› æ­¤å®ƒä¹Ÿ**åŒ¿ååŒ–äº†æ•°æ®**ã€‚

# ä¸ä¸€è‡´çš„æ ‡ç­¾ç±»åˆ«

æ•°æ®å¯èƒ½å› ä¸ºè½¬æ¢ä¸æˆåŠŸæˆ–ä»…ä»…æ˜¯äººä¸ºè¾“å…¥é”™è¯¯è€Œå‡ºç°é”™è¯¯ã€‚

å› æ­¤ï¼Œä½ å¯èƒ½ä¼šå‘ç°**ç›¸åŒçš„æ ‡ç­¾æœ‰æ‹¼å†™é”™è¯¯**ï¼Œä¸åŒçš„**å¤§å°å†™**ï¼Œ**ç¼©å†™**ï¼Œä¾‹å¦‚ï¼š_BLUE, Blue, b, bule_ã€‚åœ¨è®­ç»ƒæ¨¡å‹ä¹‹å‰ï¼Œä½ éœ€è¦ä¿®æ­£æ•°æ®ä¸­çš„è¿™äº›æ ‡ç­¾é”™è¯¯ã€‚

ä½ å¯ä»¥é€šè¿‡å°†æ‰€æœ‰å†…å®¹è½¬æ¢ä¸ºå°å†™å¹¶å°†æ‹¼å†™é”™è¯¯çš„æ ‡ç­¾æ˜ å°„åˆ°æ­£ç¡®çš„æ ‡ç­¾æ¥æ¸…ç†è¿™äº›é—®é¢˜ã€‚

éå¸¸é‡è¦çš„æ˜¯è¦æ£€æŸ¥**ä½ æ‹¥æœ‰çš„æ‰€æœ‰æ•°æ®æ˜¯å¦éƒ½è¢«æ­£ç¡®æ ‡è®°**ï¼Œå› ä¸ºä¾‹å¦‚ï¼Œæ•°æ®ä¸­çš„ä¸€ä¸ªæ‹¼å†™é”™è¯¯ï¼Œåœ¨å¯¹ç±»åˆ«è¿›è¡Œè™šæ‹Ÿç¼–ç æ—¶ï¼Œå°†åœ¨æœ€ç»ˆç‰¹å¾ä¸­ç”Ÿæˆä¸€ä¸ªæ–°åˆ—ï¼Œ**å¯¹æœ€ç»ˆæ¨¡å‹äº§ç”Ÿä¸è‰¯åæœ**ã€‚è¿™ä¸ªä¾‹å­å¯ä»¥é€šè¿‡å¯¹ä¸€åˆ—è¿›è¡Œç‹¬çƒ­ç¼–ç å¹¶æ£€æŸ¥åˆ›å»ºçš„åˆ—çš„åç§°æ¥å¾ˆå®¹æ˜“åœ°æ£€æµ‹åˆ°ã€‚

# ç¼ºå¤±æ•°æ®

ç ”ç©¶ä¸­çš„æŸäº›æ•°æ®å¯èƒ½ç¼ºå¤±ã€‚

å¯èƒ½å‘ç”ŸæŸäº›å®Œå…¨éšæœºçš„æ•°æ®å› æŸäº›é”™è¯¯è€Œç¼ºå¤±ã€‚è¿™ç§æ•°æ®æ˜¯**å®Œå…¨éšæœºç¼ºå¤±**ï¼ˆ**MCAR**ï¼‰ã€‚

å¯èƒ½æŸäº›éšæœºæ•°æ®ç¼ºå¤±ï¼Œä½†æœ‰æŸäº›å› ç´ ä½¿å¾—æŸäº›ç‰¹å®šç»†èŠ‚æ›´å¯èƒ½ç¼ºå¤±ï¼Œä¾‹å¦‚ï¼Œç”·æ€§æ›´é¢‘ç¹åœ°æŠ¥å‘Šä»–ä»¬çš„å¹´é¾„ï¼Œè€Œå¥³æ€§åˆ™ä¸ç„¶ã€‚è¿™è¢«ç§°ä¸º**éšæœºç¼ºå¤±**ï¼ˆ**MAR**ï¼‰ã€‚

æœ€åï¼Œå¯èƒ½æœ‰æ•°æ®**ééšæœºç¼ºå¤±**ï¼ˆ**MNAR**ï¼‰ã€‚æ•°æ®çš„ä»·å€¼ç›´æ¥ä¸æ‹¥æœ‰æ•°æ®çš„æ¦‚ç‡ç›¸å…³ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ æƒ³æµ‹é‡ä¸€äº›å°´å°¬çš„äº‹æƒ…ï¼ŒæŸäººè¶Šå°´å°¬ï¼Œä»–åˆ†äº«çš„å¯èƒ½æ€§å°±è¶Šå°ã€‚

å‰**ä¸¤ç±»**ç¼ºå¤±æ•°æ®å¯ä»¥æ˜¯**å¯ä»¥å¿½ç•¥çš„**ã€‚ä½†æ˜¯**ç¬¬ä¸‰ç±»**éœ€è¦è€ƒè™‘**åªä½¿ç”¨æœªå—å½±å“çš„æ•°æ®éƒ¨åˆ†**ï¼Œæˆ–å°è¯•**ä»¥æŸç§æ–¹å¼å¯¹ç¼ºå¤±æ•°æ®è¿›è¡Œå»ºæ¨¡**ã€‚

å‘ç°ç¼ºå¤±æ•°æ®çš„ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨`.info()`å‡½æ•°ï¼Œå› ä¸ºå®ƒä¼šæŒ‡ç¤º**è¡Œæ•°ä½†ä¹Ÿä¼šæŒ‡ç¤ºæ¯ä¸ªç±»åˆ«çš„å€¼çš„æ•°é‡**ã€‚å¦‚æœæŸä¸ªç±»åˆ«çš„å€¼å°‘äºè¡Œæ•°ï¼Œé‚£ä¹ˆå°±æœ‰ä¸€äº›æ•°æ®ç¼ºå¤±ï¼š
```bash
# Get info of the dataset
dataset.info()

# Drop all rows where some value is missing
dataset.dropna(how='any', axis=0).info()
```
é€šå¸¸å»ºè®®ï¼Œå¦‚æœä¸€ä¸ªç‰¹å¾åœ¨æ•°æ®é›†ä¸­**ç¼ºå¤±è¶…è¿‡20%**ï¼Œåˆ™åº”è¯¥**ç§»é™¤è¯¥åˆ—ï¼š**
```bash
# Remove column
dataset.drop('Column_name', axis='columns', inplace=True)
dataset.info()
```
{% hint style="info" %}
è¯·æ³¨æ„ï¼Œå¹¶éæ‰€æœ‰ç¼ºå¤±å€¼éƒ½åœ¨æ•°æ®é›†ä¸­ç¼ºå¤±ã€‚å¯èƒ½ç¼ºå¤±å€¼è¢«èµ‹äºˆäº†â€œUnknownâ€ã€â€œn/aâ€ã€â€œâ€ï¼Œ-1ï¼Œ0ç­‰å€¼ã€‚æ‚¨éœ€è¦æ£€æŸ¥æ•°æ®é›†ï¼ˆä½¿ç”¨`dataset.column_name.value_counts(dropna=False)`æ¥æ£€æŸ¥å¯èƒ½çš„å€¼ï¼‰ã€‚
{% endhint %}

å¦‚æœæ•°æ®é›†ä¸­æŸäº›æ•°æ®ç¼ºå¤±ï¼ˆå¹¶ä¸”æ•°é‡ä¸å¤šï¼‰ï¼Œæ‚¨éœ€è¦æ‰¾åˆ°**ç¼ºå¤±æ•°æ®çš„ç±»åˆ«**ã€‚ä¸ºæ­¤ï¼Œæ‚¨åŸºæœ¬ä¸Šéœ€è¦çŸ¥é“**æ•°æ®æ˜¯å¦æ˜¯éšæœºç¼ºå¤±çš„**ï¼Œä¸ºæ­¤æ‚¨éœ€è¦æ‰¾å‡º**ç¼ºå¤±æ•°æ®æ˜¯å¦ä¸æ•°æ®é›†ä¸­çš„å…¶ä»–æ•°æ®ç›¸å…³**ã€‚

è¦æ‰¾å‡ºç¼ºå¤±å€¼æ˜¯å¦ä¸å¦ä¸€åˆ—ç›¸å…³ï¼Œæ‚¨å¯ä»¥åˆ›å»ºä¸€ä¸ªæ–°åˆ—ï¼Œå¦‚æœæ•°æ®ç¼ºå¤±åˆ™æ ‡è®°ä¸º1ï¼Œå¦åˆ™ä¸º0ï¼Œç„¶åè®¡ç®—å®ƒä»¬ä¹‹é—´çš„ç›¸å…³æ€§ï¼š
```bash
# The closer it's to 1 or -1 the more correlated the data is
# Note that columns are always perfectly correlated with themselves.
dataset[['column_name', 'cloumn_missing_data']].corr()
```
å¦‚æœæ‚¨å†³å®šå¿½ç•¥ç¼ºå¤±æ•°æ®ï¼Œæ‚¨ä»ç„¶éœ€è¦å¤„ç†å®ƒï¼šæ‚¨å¯ä»¥**åˆ é™¤**åŒ…å«ç¼ºå¤±æ•°æ®çš„è¡Œï¼ˆæ¨¡å‹çš„è®­ç»ƒæ•°æ®ä¼šå‡å°‘ï¼‰ï¼Œæ‚¨å¯ä»¥**å®Œå…¨åˆ é™¤è¯¥ç‰¹å¾**ï¼Œæˆ–è€…å¯ä»¥**å¯¹å…¶å»ºæ¨¡**ã€‚

æ‚¨åº”è¯¥**æ£€æŸ¥ç¼ºå¤±ç‰¹å¾ä¸ç›®æ ‡åˆ—ä¹‹é—´çš„ç›¸å…³æ€§**ï¼Œä»¥äº†è§£è¯¥ç‰¹å¾å¯¹ç›®æ ‡çš„é‡è¦æ€§ï¼Œå¦‚æœå®ƒçœŸçš„**å¾ˆå°**ï¼Œæ‚¨å¯ä»¥**åˆ é™¤å®ƒæˆ–å¡«å……å®ƒ**ã€‚

è¦å¡«è¡¥ç¼ºå¤±çš„**è¿ç»­æ•°æ®**ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ï¼š**å¹³å‡å€¼**ã€**ä¸­ä½æ•°**æˆ–ä½¿ç”¨**æ’è¡¥**ç®—æ³•ã€‚æ’è¡¥ç®—æ³•å¯ä»¥å°è¯•ä½¿ç”¨å…¶ä»–ç‰¹å¾æ¥æ‰¾åˆ°ç¼ºå¤±ç‰¹å¾çš„å€¼ï¼š
```python
from sklearn.impute import KNNImputer

X = dataset[['column1', 'column2', 'column3']]
y = dataset.column_target

# Create the imputer that will fill the data
imputer = KNNImputer(n_neightbors=2, weights='uniform')
X_imp = imputer.fit_transform(X)

# Check new data
dataset_imp = pd.DataFrame(X_imp)
dataset.columns = ['column1', 'column2', 'column3']
dataset.iloc[10:20] # Get some indexes that contained empty data before
```
# ç»“åˆç‰¹å¾

å¦‚æœä½ å‘ç°**ä¸¤ä¸ªç‰¹å¾**ä¹‹é—´å­˜åœ¨**ç›¸å…³æ€§**ï¼Œé€šå¸¸ä½ åº”è¯¥**åˆ é™¤**å…¶ä¸­ä¸€ä¸ªï¼ˆä¸ç›®æ ‡ç›¸å…³æ€§è¾ƒä½çš„é‚£ä¸ªï¼‰ï¼Œä½†ä½ ä¹Ÿå¯ä»¥å°è¯•**ç»“åˆå®ƒä»¬å¹¶åˆ›å»ºä¸€ä¸ªæ–°ç‰¹å¾**ã€‚
```python
# Create a new feautr combining feature1 and feature2
dataset['new_feature'] = dataset.column1/dataset.column2

# Check correlation with target column
dataset[['new_feature', 'column1', 'column2', 'target']].corr()['target'][:]

# Check for collinearity of the 2 features and the new one
X = add_constant(dataset[['column1', 'column2', 'target']])
# Calculate VIF
pd.Series([variance_inflation_factor(X.values, i) for i in range(X.shape[1])], index=X.columns)
```
<details>

<summary><strong>ä»é›¶å¼€å§‹å­¦ä¹ AWSé»‘å®¢æŠ€æœ¯ï¼Œæˆä¸º</strong> <a href="https://training.hacktricks.xyz/courses/arte"><strong>htARTE (HackTricks AWS Red Team Expert)</strong></a><strong>ï¼</strong></summary>

æ”¯æŒHackTricksçš„å…¶ä»–æ–¹å¼ï¼š

* å¦‚æœæ‚¨æƒ³åœ¨**HackTricksä¸­çœ‹åˆ°æ‚¨çš„å…¬å¸å¹¿å‘Š**æˆ–**ä¸‹è½½HackTricksçš„PDFç‰ˆæœ¬**ï¼Œè¯·æŸ¥çœ‹[**è®¢é˜…è®¡åˆ’**](https://github.com/sponsors/carlospolop)ï¼
* è·å–[**å®˜æ–¹PEASS & HackTrickså•†å“**](https://peass.creator-spring.com)
* å‘ç°[**PEASSå®¶æ—**](https://opensea.io/collection/the-peass-family)ï¼Œæˆ‘ä»¬ç‹¬å®¶çš„[**NFTsç³»åˆ—**](https://opensea.io/collection/the-peass-family)
* **åŠ å…¥** ğŸ’¬ [**Discordç¾¤ç»„**](https://discord.gg/hRep4RUj7f) æˆ– [**telegramç¾¤ç»„**](https://t.me/peass) æˆ–åœ¨ **Twitter** ğŸ¦ ä¸Š**å…³æ³¨**æˆ‘ [**@carlospolopm**](https://twitter.com/carlospolopm)**ã€‚**
* **é€šè¿‡å‘** [**HackTricks**](https://github.com/carlospolop/hacktricks) å’Œ [**HackTricks Cloud**](https://github.com/carlospolop/hacktricks-cloud) githubä»“åº“æäº¤PRæ¥åˆ†äº«æ‚¨çš„é»‘å®¢æŠ€å·§ã€‚

</details>
