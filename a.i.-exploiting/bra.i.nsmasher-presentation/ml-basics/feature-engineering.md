<details>

<summary><strong>ä»é›¶å¼€å§‹å­¦ä¹ AWSé»‘å®¢æŠ€æœ¯ï¼Œæˆä¸ºä¸“å®¶</strong> <a href="https://training.hacktricks.xyz/courses/arte"><strong>htARTEï¼ˆHackTricks AWSçº¢é˜Ÿä¸“å®¶ï¼‰</strong></a><strong>ï¼</strong></summary>

å…¶ä»–æ”¯æŒHackTricksçš„æ–¹å¼ï¼š

* å¦‚æœæ‚¨æƒ³çœ‹åˆ°æ‚¨çš„**å…¬å¸åœ¨HackTricksä¸­åšå¹¿å‘Š**æˆ–**ä¸‹è½½PDFæ ¼å¼çš„HackTricks**ï¼Œè¯·æŸ¥çœ‹[**è®¢é˜…è®¡åˆ’**](https://github.com/sponsors/carlospolop)!
* è·å–[**å®˜æ–¹PEASS & HackTrickså‘¨è¾¹äº§å“**](https://peass.creator-spring.com)
* æ¢ç´¢[**PEASSå®¶æ—**](https://opensea.io/collection/the-peass-family)ï¼Œæˆ‘ä»¬çš„ç‹¬å®¶[**NFTs**](https://opensea.io/collection/the-peass-family)
* **åŠ å…¥** ğŸ’¬ [**Discordç¾¤ç»„**](https://discord.gg/hRep4RUj7f) æˆ– [**ç”µæŠ¥ç¾¤ç»„**](https://t.me/peass) æˆ– **å…³æ³¨**æˆ‘ä»¬çš„**Twitter** ğŸ¦ [**@hacktricks_live**](https://twitter.com/hacktricks_live)**ã€‚**
* é€šè¿‡å‘[**HackTricks**](https://github.com/carlospolop/hacktricks)å’Œ[**HackTricks Cloud**](https://github.com/carlospolop/hacktricks-cloud) githubä»“åº“æäº¤PRæ¥åˆ†äº«æ‚¨çš„é»‘å®¢æŠ€å·§ã€‚

</details>


# å¯èƒ½æ•°æ®çš„åŸºæœ¬ç±»å‹

æ•°æ®å¯ä»¥æ˜¯**è¿ç»­çš„**ï¼ˆ**æ— é™**ä¸ªå€¼ï¼‰æˆ–**åˆ†ç±»çš„**ï¼ˆåä¹‰ï¼‰ï¼Œå…¶ä¸­å¯èƒ½çš„å€¼çš„æ•°é‡æ˜¯**æœ‰é™**çš„ã€‚

## åˆ†ç±»ç±»å‹

### äºŒå…ƒ

åªæœ‰**2ä¸ªå¯èƒ½çš„å€¼**ï¼š1æˆ–0ã€‚å¦‚æœæ•°æ®é›†ä¸­çš„å€¼æ˜¯å­—ç¬¦ä¸²æ ¼å¼ï¼ˆä¾‹å¦‚"True"å’Œ"False"ï¼‰ï¼Œæ‚¨å¯ä»¥ä¸ºè¿™äº›å€¼åˆ†é…æ•°å­—ï¼š
```python
dataset["column2"] = dataset.column2.map({"T": 1, "F": 0})
```
### **åºæ•°**

**æ•°å€¼éµå¾ªä¸€å®šé¡ºåº**ï¼Œæ¯”å¦‚ï¼šç¬¬1åï¼Œç¬¬2å... å¦‚æœç±»åˆ«æ˜¯å­—ç¬¦ä¸²ï¼ˆæ¯”å¦‚ï¼š"åˆå­¦è€…"ï¼Œ"ä¸šä½™çˆ±å¥½è€…"ï¼Œ"ä¸“ä¸šäººå£«"ï¼Œ"ä¸“å®¶"ï¼‰ï¼Œä½ å¯ä»¥å°†å®ƒä»¬æ˜ å°„ä¸ºæ•°å­—ï¼Œå°±åƒæˆ‘ä»¬åœ¨äºŒè¿›åˆ¶æƒ…å†µä¸‹çœ‹åˆ°çš„é‚£æ ·ã€‚
```python
column2_mapping = {'starter':0,'amateur':1,'professional':2,'expert':3}
dataset['column2'] = dataset.column2.map(column2_mapping)
```
* å¯¹äº**å­—æ¯åˆ—**ï¼Œæ‚¨å¯ä»¥æ›´è½»æ¾åœ°å¯¹å…¶è¿›è¡Œæ’åºï¼š
```python
# First get all the uniq values alphabetically sorted
possible_values_sorted = dataset.column2.sort_values().unique().tolist()
# Assign each one a value
possible_values_mapping = {value:idx for idx,value in enumerate(possible_values_sorted)}
dataset['column2'] = dataset.column2.map(possible_values_mapping)
```
### **å¾ªç¯çš„**

çœ‹èµ·æ¥åƒåºæ•°å€¼å› ä¸ºæœ‰ä¸€ä¸ªé¡ºåºï¼Œä½†è¿™å¹¶ä¸æ„å‘³ç€ä¸€ä¸ªæ¯”å¦ä¸€ä¸ªå¤§ã€‚å¦å¤–ï¼Œå®ƒä»¬ä¹‹é—´çš„è·ç¦»å–å†³äºä½ è®¡æ•°çš„æ–¹å‘ã€‚ä¾‹å¦‚ï¼šä¸€å‘¨ä¸­çš„æ¯ä¸€å¤©ï¼Œæ˜ŸæœŸæ—¥å¹¶ä¸æ¯”æ˜ŸæœŸä¸€â€œå¤§â€ã€‚

- æœ‰ä¸åŒçš„æ–¹æ³•æ¥ç¼–ç å¾ªç¯ç‰¹å¾ï¼Œæœ‰äº›å¯èƒ½åªé€‚ç”¨äºæŸäº›ç®—æ³•ã€‚**é€šå¸¸æƒ…å†µä¸‹ï¼Œå¯ä»¥ä½¿ç”¨è™šæ‹Ÿç¼–ç **ã€‚
```python
column2_dummies = pd.get_dummies(dataset.column2, drop_first=True)
dataset_joined = pd.concat([dataset[['column2']], column2_dummies], axis=1)
```
### **æ—¥æœŸ**

æ—¥æœŸæ˜¯**è¿ç»­**çš„**å˜é‡**ã€‚å¯ä»¥è¢«è§†ä¸º**å¾ªç¯**çš„ï¼ˆå› ä¸ºå®ƒä»¬é‡å¤ï¼‰**æˆ–**ä½œä¸º**åºæ•°**å˜é‡ï¼ˆå› ä¸ºä¸€ä¸ªæ—¶é—´æ¯”å‰ä¸€ä¸ªæ—¶é—´å¤§ï¼‰ã€‚

* é€šå¸¸æ—¥æœŸè¢«ç”¨ä½œ**ç´¢å¼•**
```python
# Transform dates to datetime
dataset["column_date"] = pd.to_datetime(dataset.column_date)
# Make the date feature the index
dataset.set_index('column_date', inplace=True)
print(dataset.head())

# Sum usage column per day
daily_sum = dataset.groupby(df_daily_usage.index.date).agg({'usage':['sum']})
# Flatten and rename usage column
daily_sum.columns = daily_sum.columns.get_level_values(0)
daily_sum.columns = ['daily_usage']
print(daily_sum.head())

# Fill days with 0 usage
idx = pd.date_range('2020-01-01', '2020-12-31')
daily_sum.index = pd.DatetimeIndex(daily_sum.index)
df_filled = daily_sum.reindex(idx, fill_value=0) # Fill missing values


# Get day of the week, Monday=0, Sunday=6, and week days names
dataset['DoW'] = dataset.transaction_date.dt.dayofweek
# do the same in a different way
dataset['weekday'] = dataset.transaction_date.dt.weekday
# get day names
dataset['day_name'] = dataset.transaction_date.apply(lambda x: x.day_name())
```
### å¤šç±»åˆ«/åä¹‰

**è¶…è¿‡2ä¸ªç±»åˆ«**ï¼Œæ²¡æœ‰ç›¸å…³é¡ºåºã€‚ä½¿ç”¨ `dataset.describe(include='all')` è·å–æ¯ä¸ªç‰¹å¾çš„ç±»åˆ«ä¿¡æ¯ã€‚

* **å¼•ç”¨å­—ç¬¦ä¸²**æ˜¯**æ ‡è¯†ç¤ºä¾‹çš„åˆ—**ï¼ˆå¦‚äººåï¼‰ã€‚è¿™å¯èƒ½æ˜¯é‡å¤çš„ï¼ˆå› ä¸ºä¸¤ä¸ªäººå¯èƒ½æœ‰ç›¸åŒçš„åå­—ï¼‰ï¼Œä½†å¤§å¤šæ•°æ˜¯å”¯ä¸€çš„ã€‚è¿™äº›æ•°æ®æ˜¯**æ— ç”¨çš„ï¼Œåº”è¯¥è¢«ç§»é™¤**ã€‚
* **å…³é”®åˆ—**ç”¨äº**é“¾æ¥è¡¨æ ¼ä¹‹é—´çš„æ•°æ®**ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå…ƒç´ æ˜¯å”¯ä¸€çš„ã€‚è¿™äº›æ•°æ®æ˜¯**æ— ç”¨çš„ï¼Œåº”è¯¥è¢«ç§»é™¤**ã€‚

ä¸ºäº†**å°†å¤šç±»åˆ«åˆ—ç¼–ç ä¸ºæ•°å­—**ï¼ˆä»¥ä¾¿æœºå™¨å­¦ä¹ ç®—æ³•ç†è§£å®ƒä»¬ï¼‰ï¼Œä½¿ç”¨**è™šæ‹Ÿç¼–ç **ï¼ˆè€Œä¸æ˜¯ç‹¬çƒ­ç¼–ç ï¼Œå› ä¸ºå®ƒ**ä¸èƒ½é¿å…å®Œç¾çš„å¤šé‡å…±çº¿æ€§**ï¼‰ã€‚

æ‚¨å¯ä»¥ä½¿ç”¨ `pd.get_dummies(dataset.column1)` **å¯¹å¤šç±»åˆ«åˆ—è¿›è¡Œç‹¬çƒ­ç¼–ç **ã€‚è¿™å°†æŠŠæ‰€æœ‰ç±»åˆ«è½¬æ¢ä¸ºäºŒè¿›åˆ¶ç‰¹å¾ï¼Œå› æ­¤å°†ä¸ºæ¯ä¸ªå¯èƒ½çš„ç±»åˆ«åˆ›å»º**ä¸€ä¸ªæ–°åˆ—**ï¼Œå¹¶å°†ä¸€ä¸ª**Trueå€¼åˆ†é…ç»™ä¸€ä¸ªåˆ—**ï¼Œå…¶ä½™å°†ä¸ºfalseã€‚

æ‚¨å¯ä»¥ä½¿ç”¨ `pd.get_dummies(dataset.column1, drop_first=True)` **å¯¹å¤šç±»åˆ«åˆ—è¿›è¡Œè™šæ‹Ÿç¼–ç **ã€‚è¿™å°†æŠŠæ‰€æœ‰ç±»åˆ«è½¬æ¢ä¸ºäºŒè¿›åˆ¶ç‰¹å¾ï¼Œå› æ­¤å°†ä¸ºæ¯ä¸ªå¯èƒ½çš„ç±»åˆ«åˆ›å»º**ä¸€ä¸ªæ–°åˆ—å‡å»ä¸€ä¸ª**ï¼Œå› ä¸º**æœ€åä¸¤åˆ—å°†åœ¨æœ€ååˆ›å»ºçš„äºŒè¿›åˆ¶åˆ—ä¸­åæ˜ ä¸º"1"æˆ–"0"**ã€‚è¿™å°†é¿å…å®Œç¾çš„å¤šé‡å…±çº¿æ€§ï¼Œå‡å°‘åˆ—ä¹‹é—´çš„å…³ç³»ã€‚

# å…±çº¿æ€§/å¤šé‡å…±çº¿æ€§

å½“**2ä¸ªç‰¹å¾å½¼æ­¤ç›¸å…³**æ—¶å‡ºç°å…±çº¿æ€§ã€‚å½“è¿™äº›ç‰¹å¾è¶…è¿‡2ä¸ªæ—¶å‡ºç°å¤šé‡å…±çº¿æ€§ã€‚

åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œ**æ‚¨å¸Œæœ›æ‚¨çš„ç‰¹å¾ä¸å¯èƒ½çš„ç»“æœç›¸å…³ï¼Œä½†ä¸å¸Œæœ›å®ƒä»¬ä¹‹é—´ç›¸å…³**ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆ**è™šæ‹Ÿç¼–ç æ··åˆæœ€åä¸¤åˆ—**ï¼Œæ¯”ç‹¬çƒ­ç¼–ç æ›´å¥½ï¼Œåè€…ä¸ä¼šåˆ›å»ºæ¸…æ™°çš„å…³ç³»ï¼Œè€Œæ˜¯åˆ›å»ºäº†æ¥è‡ªå¤šç±»åˆ«åˆ—çš„æ‰€æœ‰æ–°ç‰¹å¾ä¹‹é—´çš„æ˜æ˜¾å…³ç³»ã€‚

VIFæ˜¯**æ–¹å·®è†¨èƒ€å› å­**ï¼Œç”¨äº**è¡¡é‡ç‰¹å¾çš„å¤šé‡å…±çº¿æ€§**ã€‚å€¼**å¤§äº5æ„å‘³ç€åº”è¯¥ç§»é™¤ä¸¤ä¸ªæˆ–å¤šä¸ªå…±çº¿ç‰¹å¾ä¸­çš„ä¸€ä¸ª**ã€‚
```python
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

#dummies_encoded = pd.get_dummies(dataset.column1, drop_first=True)
onehot_encoded = pd.get_dummies(dataset.column1)
X = add_constant(onehot_encoded) # Add previously one-hot encoded data
print(pd.Series([variance_inflation_factor(X.values,i) for i in range(X.shape[1])], index=X.columns))
```
# ç±»åˆ«ä¸å¹³è¡¡

è¿™ç§æƒ…å†µå‘ç”Ÿåœ¨è®­ç»ƒæ•°æ®ä¸­**æ¯ä¸ªç±»åˆ«çš„æ•°é‡ä¸ç›¸åŒ**æ—¶ã€‚
```python
# Get statistic of the features
print(dataset.describe(include='all'))
# Get an overview of the features
print(dataset.info())
# Get imbalance information of the target column
print(dataset.target_column.value_counts())
```
åœ¨ä¸å¹³è¡¡çš„æƒ…å†µä¸‹ï¼Œæ€»ä¼šå­˜åœ¨**å¤šæ•°ç±»åˆ«**å’Œ**å°‘æ•°ç±»åˆ«**ã€‚

è§£å†³è¿™ä¸ªé—®é¢˜æœ‰ä¸¤ç§ä¸»è¦æ–¹æ³•ï¼š

- **æ¬ é‡‡æ ·**ï¼šä»å¤šæ•°ç±»åˆ«ä¸­éšæœºåˆ é™¤æ•°æ®ï¼Œä½¿å…¶æ ·æœ¬æ•°é‡ä¸å°‘æ•°ç±»åˆ«ç›¸åŒã€‚
```python
from imblearn.under_sampling import RandomUnderSampler
rus = RandomUserSampler(random_state=1337)

X = dataset[['column1', 'column2', 'column3']].copy()
y = dataset.target_column

X_under, y_under = rus.fit_resample(X,y)
print(y_under.value_counts()) #Confirm data isn't imbalanced anymore
```
* **è¿‡é‡‡æ ·**ï¼šä¸ºå°‘æ•°ç±»ç”Ÿæˆæ›´å¤šæ•°æ®ï¼Œç›´åˆ°å…¶æ ·æœ¬æ•°é‡ä¸å¤šæ•°ç±»ç›¸åŒã€‚
```python
from imblearn.under_sampling import RandomOverSampler
ros = RandomOverSampler(random_state=1337)

X = dataset[['column1', 'column2', 'column3']].copy()
y = dataset.target_column

X_over, y_over = ros.fit_resample(X,y)
print(y_over.value_counts()) #Confirm data isn't imbalanced anymore
```
æ‚¨å¯ä»¥ä½¿ç”¨å‚æ•°**`sampling_strategy`**æ¥æŒ‡ç¤ºæ‚¨æƒ³è¦**æ¬ é‡‡æ ·æˆ–è¿‡é‡‡æ ·çš„ç™¾åˆ†æ¯”**ï¼ˆ**é»˜è®¤ä¸º1ï¼ˆ100%ï¼‰**ï¼Œè¿™æ„å‘³ç€å°†å°‘æ•°ç±»çš„æ•°é‡ä¸å¤šæ•°ç±»ç›¸ç­‰ï¼‰

{% hint style="info" %}
å¦‚æœæ‚¨è·å–è¿‡/æ¬ é‡‡æ ·æ•°æ®çš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆä½¿ç”¨`.describe()`ï¼‰ï¼Œå¹¶å°†å…¶ä¸åŸå§‹æ•°æ®è¿›è¡Œæ¯”è¾ƒï¼Œæ‚¨ä¼šå‘ç°**å®ƒä»¬å·²ç»æ”¹å˜**ã€‚å› æ­¤ï¼Œè¿‡é‡‡æ ·å’Œæ¬ é‡‡æ ·ä¼šä¿®æ”¹è®­ç»ƒæ•°æ®ã€‚
{% endhint %}

## SMOTE è¿‡é‡‡æ ·

**SMOTE**é€šå¸¸æ˜¯ä¸€ç§**æ›´å¯é çš„è¿‡é‡‡æ ·æ•°æ®çš„æ–¹æ³•**ã€‚
```python
from imblearn.over_sampling import SMOTE

# Form SMOTE the target_column need to be numeric, map it if necessary
smote = SMOTE(random_state=1337)
X_smote, y_smote = smote.fit_resample(dataset[['column1', 'column2', 'column3']], dataset.target_column)
dataset_smote = pd.DataFrame(X_smote, columns=['column1', 'column2', 'column3'])
dataset['target_column'] = y_smote
print(y_smote.value_counts()) #Confirm data isn't imbalanced anymore
```
# å¾ˆå°‘å‡ºç°çš„ç±»åˆ«

æƒ³è±¡ä¸€ä¸‹ä¸€ä¸ªæ•°æ®é›†ï¼Œå…¶ä¸­ä¸€ä¸ªç›®æ ‡ç±»åˆ«**å‡ºç°çš„æ¬¡æ•°éå¸¸å°‘**ã€‚

è¿™å°±åƒå‰ä¸€èŠ‚ä¸­çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œä½†æ˜¯å¾ˆå°‘å‡ºç°çš„ç±»åˆ«ç”šè‡³æ¯”é‚£ç§æƒ…å†µä¸‹çš„â€œå°‘æ•°ç±»â€å‡ºç°çš„æ¬¡æ•°è¿˜è¦å°‘ã€‚**åŸå§‹**çš„**è¿‡é‡‡æ ·**å’Œ**æ¬ é‡‡æ ·**æ–¹æ³•ä¹Ÿå¯ä»¥åœ¨è¿™é‡Œä½¿ç”¨ï¼Œä½†é€šå¸¸è¿™äº›æŠ€æœ¯**ä¸ä¼šç»™å‡ºçœŸæ­£å¥½çš„ç»“æœ**ã€‚

## æƒé‡

åœ¨ä¸€äº›ç®—æ³•ä¸­ï¼Œå¯ä»¥**ä¿®æ”¹ç›®æ ‡æ•°æ®çš„æƒé‡**ï¼Œè¿™æ ·åœ¨ç”Ÿæˆæ¨¡å‹æ—¶ï¼Œä¸€äº›æ•°æ®ä¼šé»˜è®¤è·å¾—æ›´å¤šçš„é‡è¦æ€§ã€‚
```python
weights = {0: 10 1:1} #Assign weight 10 to False and 1 to True
model = LogisticRegression(class_weight=weights)
```
## ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰

æ˜¯ä¸€ç§æœ‰åŠ©äºå‡å°‘æ•°æ®ç»´åº¦çš„æ–¹æ³•ã€‚å®ƒå°†**ç»“åˆä¸åŒç‰¹å¾**ä»¥**å‡å°‘**å®ƒä»¬çš„æ•°é‡ï¼Œç”Ÿæˆ**æ›´æœ‰ç”¨çš„ç‰¹å¾**ï¼ˆ_éœ€è¦æ›´å°‘çš„è®¡ç®—_ï¼‰ã€‚

ç”Ÿæˆçš„ç‰¹å¾å¯¹äººç±»æ¥è¯´æ˜¯ä¸å¯ç†è§£çš„ï¼Œå› æ­¤å®ƒè¿˜**å¯¹æ•°æ®è¿›è¡ŒåŒ¿ååŒ–**ã€‚

# ä¸ä¸€è‡´çš„æ ‡ç­¾ç±»åˆ«

æ•°æ®å¯èƒ½å­˜åœ¨é”™è¯¯ï¼Œå¯èƒ½æ˜¯ç”±äºè½¬æ¢å¤±è´¥ï¼Œä¹Ÿå¯èƒ½æ˜¯ç”±äºäººä¸ºé”™è¯¯ã€‚

å› æ­¤ï¼Œæ‚¨å¯èƒ½ä¼šå‘ç°**ç›¸åŒæ ‡ç­¾å­˜åœ¨æ‹¼å†™é”™è¯¯**ã€**ä¸åŒå¤§å°å†™**ã€**ç¼©å†™**ï¼Œä¾‹å¦‚ï¼š_BLUEï¼ŒBlueï¼Œbï¼Œbuleã€‚åœ¨è®­ç»ƒæ¨¡å‹ä¹‹å‰ï¼Œæ‚¨éœ€è¦ä¿®å¤æ•°æ®ä¸­çš„è¿™äº›æ ‡ç­¾é”™è¯¯ã€‚

æ‚¨å¯ä»¥é€šè¿‡å°†æ‰€æœ‰å†…å®¹è½¬æ¢ä¸ºå°å†™å¹¶å°†æ‹¼å†™é”™è¯¯çš„æ ‡ç­¾æ˜ å°„åˆ°æ­£ç¡®çš„æ ‡ç­¾æ¥æ¸…ç†è¿™äº›é—®é¢˜ã€‚

éå¸¸é‡è¦çš„æ˜¯è¦æ£€æŸ¥**æ‚¨æ‹¥æœ‰çš„æ‰€æœ‰æ•°æ®æ˜¯å¦è¢«æ­£ç¡®æ ‡è®°**ï¼Œå› ä¸ºä¾‹å¦‚ï¼Œæ•°æ®ä¸­çš„ä¸€ä¸ªæ‹¼å†™é”™è¯¯ï¼Œåœ¨å¯¹ç±»åˆ«è¿›è¡Œè™šæ‹Ÿç¼–ç æ—¶ï¼Œå°†åœ¨æœ€ç»ˆç‰¹å¾ä¸­ç”Ÿæˆä¸€ä¸ªæ–°åˆ—ï¼Œå¯¹æœ€ç»ˆæ¨¡å‹ä¼šäº§ç”Ÿ**ä¸è‰¯åæœ**ã€‚é€šè¿‡å¯¹ä¸€ä¸ªåˆ—è¿›è¡Œç‹¬çƒ­ç¼–ç å¹¶æ£€æŸ¥æ‰€åˆ›å»ºåˆ—çš„åç§°ï¼Œå¯ä»¥éå¸¸å®¹æ˜“åœ°æ£€æµ‹åˆ°è¿™ç§æƒ…å†µã€‚

# ç¼ºå¤±æ•°æ®

ç ”ç©¶ä¸­å¯èƒ½ä¼šç¼ºå°‘ä¸€äº›æ•°æ®ã€‚

å¯èƒ½ä¼šå‘ç”Ÿä¸€äº›å®Œå…¨éšæœºçš„æ•°æ®ä¸¢å¤±ï¼Œè¿™æ˜¯ä¸€ç§**å®Œå…¨éšæœºç¼ºå¤±**ï¼ˆ**MCAR**ï¼‰çš„æƒ…å†µã€‚

ä¹Ÿå¯èƒ½æ˜¯ä¸€äº›éšæœºæ•°æ®ä¸¢å¤±ï¼Œä½†æœ‰ä¸€äº›å› ç´ ä½¿å¾—æŸäº›ç‰¹å®šç»†èŠ‚æ›´æœ‰å¯èƒ½ä¸¢å¤±ï¼Œä¾‹å¦‚æ›´é¢‘ç¹åœ°ç”·æ€§ä¼šå‘Šè¯‰ä»–ä»¬çš„å¹´é¾„è€Œå¥³æ€§ä¸ä¼šã€‚è¿™è¢«ç§°ä¸º**éšæœºç¼ºå¤±**ï¼ˆ**MAR**ï¼‰ã€‚

æœ€åï¼Œå¯èƒ½å­˜åœ¨**ééšæœºç¼ºå¤±**ï¼ˆ**MNAR**ï¼‰çš„æ•°æ®ã€‚æ•°æ®çš„å€¼ä¸æ‹¥æœ‰æ•°æ®çš„æ¦‚ç‡ç›´æ¥ç›¸å…³ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æƒ³æµ‹é‡æŸäº›ä»¤äººå°´å°¬çš„äº‹æƒ…ï¼ŒæŸäººè¶Šå°´å°¬ï¼Œä»–åˆ†äº«çš„å¯èƒ½æ€§å°±è¶Šå°ã€‚

**å‰ä¸¤ç±»**ç¼ºå¤±æ•°æ®å¯ä»¥è¢«**å¿½ç•¥**ã€‚ä½†**ç¬¬ä¸‰ç±»**éœ€è¦è€ƒè™‘**åªæœ‰éƒ¨åˆ†æœªå—å½±å“çš„æ•°æ®**æˆ–å°è¯•**ä»¥æŸç§æ–¹å¼å¯¹ç¼ºå¤±æ•°æ®è¿›è¡Œå»ºæ¨¡**ã€‚

å‘ç°ç¼ºå¤±æ•°æ®çš„ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨`.info()`å‡½æ•°ï¼Œå› ä¸ºå®ƒå°†æŒ‡ç¤º**æ¯ä¸ªç±»åˆ«çš„å€¼çš„æ•°é‡**ã€‚å¦‚æœæŸä¸ªç±»åˆ«çš„å€¼å°‘äºè¡Œæ•°ï¼Œåˆ™å­˜åœ¨ä¸€äº›æ•°æ®ç¼ºå¤±ï¼š
```bash
# Get info of the dataset
dataset.info()

# Drop all rows where some value is missing
dataset.dropna(how='any', axis=0).info()
```
é€šå¸¸å»ºè®®ï¼Œå¦‚æœæ•°æ®é›†ä¸­**è¶…è¿‡20%çš„æ•°æ®ç¼ºå¤±**ï¼Œåˆ™åº”**åˆ é™¤è¯¥åˆ—ï¼š**
```bash
# Remove column
dataset.drop('Column_name', axis='columns', inplace=True)
dataset.info()
```
{% hint style="info" %}
è¯·æ³¨æ„ï¼Œ**æ•°æ®é›†ä¸­å¹¶éæ‰€æœ‰ç¼ºå¤±å€¼éƒ½æ˜¯ç¼ºå¤±çš„**ã€‚å¯èƒ½ç¼ºå¤±å€¼å·²è¢«èµ‹äºˆå€¼ä¸º"Unknown"ã€"n/a"ã€""ã€-1ã€0ç­‰ã€‚æ‚¨éœ€è¦æ£€æŸ¥æ•°æ®é›†ï¼ˆä½¿ç”¨`dataset.column_name.value_counts(dropna=False)`æ¥æ£€æŸ¥å¯èƒ½çš„å€¼ï¼‰ã€‚
{% endhint %}

å¦‚æœæ•°æ®é›†ä¸­æœ‰ä¸€äº›æ•°æ®ç¼ºå¤±ï¼ˆè€Œä¸æ˜¯å¤ªå¤šï¼‰ï¼Œæ‚¨éœ€è¦æ‰¾åˆ°**ç¼ºå¤±æ•°æ®çš„ç±»åˆ«**ã€‚åŸºæœ¬ä¸Šï¼Œæ‚¨éœ€è¦çŸ¥é“**ç¼ºå¤±æ•°æ®æ˜¯å¦æ˜¯éšæœºçš„**ï¼Œä¸ºæ­¤ï¼Œæ‚¨éœ€è¦æ‰¾å‡º**ç¼ºå¤±æ•°æ®æ˜¯å¦ä¸æ•°æ®é›†çš„å…¶ä»–æ•°æ®ç›¸å…³**ã€‚

è¦æ‰¾å‡ºç¼ºå¤±å€¼æ˜¯å¦ä¸å¦ä¸€åˆ—ç›¸å…³è”ï¼Œæ‚¨å¯ä»¥åˆ›å»ºä¸€ä¸ªæ–°åˆ—ï¼Œå¦‚æœæ•°æ®ç¼ºå¤±æˆ–ä¸ç¼ºå¤±ï¼Œåˆ™å°†1å’Œ0æ”¾å…¥è¯¥åˆ—ï¼Œç„¶åè®¡ç®—å®ƒä»¬ä¹‹é—´çš„ç›¸å…³æ€§ï¼š
```bash
# The closer it's to 1 or -1 the more correlated the data is
# Note that columns are always perfectly correlated with themselves.
dataset[['column_name', 'cloumn_missing_data']].corr()
```
å¦‚æœæ‚¨å†³å®šå¿½ç•¥ç¼ºå¤±æ•°æ®ï¼Œä»ç„¶éœ€è¦å¤„ç†å®ƒï¼šæ‚¨å¯ä»¥**åˆ é™¤**å…·æœ‰ç¼ºå¤±æ•°æ®çš„è¡Œï¼ˆæ¨¡å‹çš„è®­ç»ƒæ•°æ®å°†æ›´å°‘ï¼‰ï¼Œæ‚¨å¯ä»¥å®Œå…¨**åˆ é™¤è¯¥ç‰¹å¾**ï¼Œæˆ–è€…å¯ä»¥**å¯¹å…¶è¿›è¡Œå»ºæ¨¡**ã€‚

æ‚¨åº”è¯¥**æ£€æŸ¥ç¼ºå¤±ç‰¹å¾ä¸ç›®æ ‡åˆ—ä¹‹é—´çš„ç›¸å…³æ€§**ï¼Œä»¥æŸ¥çœ‹è¯¥ç‰¹å¾å¯¹ç›®æ ‡çš„é‡è¦æ€§å¦‚ä½•ï¼Œå¦‚æœç¡®å®**å¾ˆå°**ï¼Œæ‚¨å¯ä»¥**åˆ é™¤å®ƒæˆ–å¡«å……å®ƒ**ã€‚

è¦å¡«è¡¥ç¼ºå¤±çš„**è¿ç»­æ•°æ®**ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ï¼š**å‡å€¼**ã€**ä¸­ä½æ•°**æˆ–ä½¿ç”¨**å¡«å……**ç®—æ³•ã€‚å¡«å……ç®—æ³•å¯ä»¥å°è¯•ä½¿ç”¨å…¶ä»–ç‰¹å¾æ¥æ‰¾åˆ°ç¼ºå¤±ç‰¹å¾çš„å€¼ï¼š
```python
from sklearn.impute import KNNImputer

X = dataset[['column1', 'column2', 'column3']]
y = dataset.column_target

# Create the imputer that will fill the data
imputer = KNNImputer(n_neightbors=2, weights='uniform')
X_imp = imputer.fit_transform(X)

# Check new data
dataset_imp = pd.DataFrame(X_imp)
dataset.columns = ['column1', 'column2', 'column3']
dataset.iloc[10:20] # Get some indexes that contained empty data before
```
ä¸ºäº†å¡«è¡¥åˆ†ç±»æ•°æ®ï¼Œé¦–å…ˆéœ€è¦è€ƒè™‘æ•°å€¼ç¼ºå¤±çš„åŸå› ã€‚å¦‚æœæ˜¯**ç”¨æˆ·é€‰æ‹©**ï¼ˆä»–ä»¬ä¸æƒ³æä¾›æ•°æ®ï¼‰ï¼Œä¹Ÿè®¸å¯ä»¥**åˆ›å»ºä¸€ä¸ªæ–°ç±»åˆ«**æ¥æŒ‡ç¤ºè¿™ä¸€ç‚¹ã€‚å¦‚æœæ˜¯å› ä¸ºäººä¸ºé”™è¯¯ï¼Œå¯ä»¥**åˆ é™¤è¡Œ**æˆ–**ç‰¹å¾**ï¼ˆåœ¨ä¹‹å‰æåˆ°çš„æ­¥éª¤ä¸­æ£€æŸ¥ï¼‰ï¼Œæˆ–è€…ç”¨**ä¼—æ•°å¡«å……ï¼Œå³æœ€å¸¸ç”¨çš„ç±»åˆ«**ï¼ˆä¸å»ºè®®ï¼‰ã€‚

# åˆå¹¶ç‰¹å¾

å¦‚æœå‘ç°**ä¸¤ä¸ªç‰¹å¾**ä¹‹é—´**ç›¸å…³**ï¼Œé€šå¸¸åº”è¯¥**åˆ é™¤**å…¶ä¸­ä¸€ä¸ªï¼ˆä¸ç›®æ ‡ç›¸å…³æ€§è¾ƒä½çš„é‚£ä¸ªï¼‰ï¼Œä½†ä¹Ÿå¯ä»¥å°è¯•**å°†å®ƒä»¬åˆå¹¶å¹¶åˆ›å»ºä¸€ä¸ªæ–°ç‰¹å¾**ã€‚
```python
# Create a new feautr combining feature1 and feature2
dataset['new_feature'] = dataset.column1/dataset.column2

# Check correlation with target column
dataset[['new_feature', 'column1', 'column2', 'target']].corr()['target'][:]

# Check for collinearity of the 2 features and the new one
X = add_constant(dataset[['column1', 'column2', 'target']])
# Calculate VIF
pd.Series([variance_inflation_factor(X.values, i) for i in range(X.shape[1])], index=X.columns)
```
<details>

<summary><strong>ä»é›¶å¼€å§‹å­¦ä¹ AWSé»‘å®¢æŠ€æœ¯ï¼Œæˆä¸ºä¸“å®¶</strong> <a href="https://training.hacktricks.xyz/courses/arte"><strong>htARTEï¼ˆHackTricks AWSçº¢é˜Ÿä¸“å®¶ï¼‰</strong></a><strong>ï¼</strong></summary>

å…¶ä»–æ”¯æŒHackTricksçš„æ–¹å¼ï¼š

* å¦‚æœæ‚¨æƒ³çœ‹åˆ°æ‚¨çš„**å…¬å¸åœ¨HackTricksä¸­åšå¹¿å‘Š**æˆ–**ä¸‹è½½PDFæ ¼å¼çš„HackTricks**ï¼Œè¯·æŸ¥çœ‹[**è®¢é˜…è®¡åˆ’**](https://github.com/sponsors/carlospolop)!
* è·å–[**å®˜æ–¹PEASS & HackTrickså‘¨è¾¹äº§å“**](https://peass.creator-spring.com)
* æ¢ç´¢[**PEASSå®¶æ—**](https://opensea.io/collection/the-peass-family)ï¼Œæˆ‘ä»¬çš„ç‹¬å®¶[**NFTs**](https://opensea.io/collection/the-peass-family)
* **åŠ å…¥** ğŸ’¬ [**Discordç¾¤**](https://discord.gg/hRep4RUj7f) æˆ– [**ç”µæŠ¥ç¾¤**](https://t.me/peass) æˆ– **å…³æ³¨**æˆ‘ä»¬çš„**Twitter** ğŸ¦ [**@hacktricks_live**](https://twitter.com/hacktricks_live)**ã€‚**
* é€šè¿‡å‘[**HackTricks**](https://github.com/carlospolop/hacktricks)å’Œ[**HackTricks Cloud**](https://github.com/carlospolop/hacktricks-cloud) githubä»“åº“æäº¤PRæ¥åˆ†äº«æ‚¨çš„é»‘å®¢æŠ€å·§ã€‚

</details>
